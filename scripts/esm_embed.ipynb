{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bd99e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/virtual/nitschle/conda_envs/LABind/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "from torch.utils.data import DataLoader\n",
    "from readData import readData\n",
    "import shutil\n",
    "from utils import *\n",
    "import torch\n",
    "from torch import nn\n",
    "from model import LABind\n",
    "from func_help import setALlSeed,get_std_opt\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "from sklearn.model_selection import KFold\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba3ce2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "#DEVICE = torch.device('cpu')\n",
    "DEVICE = torch.device('cuda:0')\n",
    "root_path = getRootPath()\n",
    "dataset = 'LigBind' # DS1:LigBind, DS2:GPSite, DS3:Unseen. Change to change dataset\n",
    "\n",
    "nn_config = {\n",
    "    # dataset \n",
    "    'train_file': f'{root_path}/{dataset}/label/training.fa',\n",
    "    'test_file': f'{root_path}/{dataset}/label/test.fa',\n",
    "    'valid_file': f'{root_path}/{dataset}/label/picking.fa',\n",
    "    'proj_dir': f'{root_path}/{dataset}/',\n",
    "    'lig_dict': pkl.load(open(f'{root_path}/tools/ligand.pkl', 'rb')),\n",
    "    'pdb_class':'source', # source or omegafold or esmfold\n",
    "    'dssp_max_repr': np.load(f'{root_path}/tools/dssp_max_repr.npy'),\n",
    "    'dssp_min_repr': np.load(f'{root_path}/tools/dssp_min_repr.npy'),\n",
    "    'ankh_max_repr': np.load(f'{root_path}/tools/ankh_max_repr.npy'),\n",
    "    'ankh_min_repr': np.load(f'{root_path}/tools/ankh_min_repr.npy'),\n",
    "    'ion_max_repr': np.load(f'{root_path}/tools/ion_max_repr.npy'),\n",
    "    'ion_min_repr': np.load(f'{root_path}/tools/ion_min_repr.npy'),\n",
    "    # model parameters\n",
    "    \n",
    "    'rfeat_dim':1556,\n",
    "    'ligand_dim':768, \n",
    "    'hidden_dim':256, \n",
    "    'heads':4, \n",
    "    'augment_eps':0.05, \n",
    "    'rbf_num':8, \n",
    "    'top_k':30, \n",
    "    'attn_drop':0.1, \n",
    "    'dropout':0.1, \n",
    "    'num_layers':4, \n",
    "    'lr':0.0004, \n",
    "    \n",
    "    # training parameters \n",
    "    # You can modify it according to the actual situation. \n",
    "    # Since it involves mapping the entire protein, it will consume a large amount of GPU memory.\n",
    "    'batch_size':15,\n",
    "    'max_patience':10,\n",
    "    'device_ids':[0,1], # 2*A100-40G\n",
    "}\n",
    "pretrain_path = { # Please modify \n",
    "    'esmfold_path': '../tools/esmfold_v1', # esmfold path\n",
    "    'ankh_path': '../tools/ankh-large/', # ankh path\n",
    "    'molformer_path': '../tools/MoLFormer-XL-both-10pct/', # molformer path\n",
    "    'model_path':f'{root_path}/model/Unseen/', # based on Unseen\n",
    "    'esm2_3B_path': '../tools/esm2_t36_3B_UR50D/', # esm2 3B path\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21b87bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.13it/s]\n",
      "Some weights of EsmModel were not initialized from the model checkpoint at ../tools/esm2_t36_3B_UR50D/ and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "1701it [00:00, 191839.50it/s]\n",
      "83it [00:00, 148518.44it/s]\n",
      "399it [00:00, 185514.61it/s]\n",
      "127it [00:00, 170238.61it/s]\n",
      "513it [00:00, 186988.61it/s]\n",
      "64it [00:00, 170869.16it/s]\n",
      "1421it [00:00, 211373.76it/s]\n",
      "203it [00:00, 184895.49it/s]\n",
      "69it [00:00, 167480.89it/s]\n",
      "370it [00:00, 191237.52it/s]\n",
      "235it [00:00, 183208.45it/s]\n",
      "1770it [00:00, 217742.13it/s]\n",
      "19it [00:00, 142561.32it/s]\n",
      "103it [00:00, 162900.95it/s]\n",
      "336it [00:00, 189089.78it/s]\n",
      "237it [00:00, 182830.61it/s]\n",
      "129it [00:00, 184286.52it/s]\n",
      "108it [00:00, 177432.37it/s]\n",
      "393it [00:00, 192318.45it/s]\n",
      "665it [00:00, 194234.83it/s]\n",
      "34it [00:00, 150428.62it/s]\n",
      "158it [00:00, 166969.02it/s]\n",
      "40it [00:00, 154914.28it/s]\n",
      "167it [00:00, 172185.05it/s]\n",
      "17it [00:00, 127554.86it/s]\n",
      "533it [00:00, 199390.30it/s]\n",
      "78it [00:00, 170127.78it/s]\n",
      "21it [00:00, 128584.50it/s]\n",
      "31it [00:00, 201.37it/s]\n",
      "67it [00:06, 10.87it/s]\n",
      "615it [00:57, 10.65it/s]\n",
      "7it [00:00, 15.05it/s]\n",
      "23it [00:02, 10.02it/s]\n",
      "139it [00:14,  9.68it/s]\n",
      "88it [00:08, 10.14it/s]\n",
      "54it [00:04, 13.18it/s]\n",
      "31it [00:02, 11.89it/s]\n",
      "73it [00:06, 10.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel \n",
    "from Bio import SeqIO\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrain_path['esm2_3B_path'])\n",
    "\n",
    "model = AutoModel.from_pretrained(pretrain_path['esm2_3B_path'], torch_dtype=torch.float32)\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "out_path = f\"{root_path}/{dataset}/esm3B/\"\n",
    "makeDir(out_path)\n",
    "\n",
    "fasta_path = f\"{root_path}/{dataset}/fasta/\"\n",
    "\n",
    "for file_class in os.listdir(fasta_path):\n",
    "    \n",
    "    for fasta_base_file in os.listdir(f\"{fasta_path}/{file_class}\"):\n",
    "        \n",
    "        fasta_file = f\"{fasta_path}/{file_class}/{fasta_base_file}\"\n",
    "        sequences = SeqIO.parse(fasta_file, \"fasta\")\n",
    "        \n",
    "        for record in tqdm(sequences):\n",
    "            \n",
    "            if os.path.exists(out_path+f'{record.id}.npy'):\n",
    "                continue\n",
    "            \n",
    "            ids = tokenizer(str(record.seq), return_tensors=\"pt\")\n",
    "            input_ids = ids['input_ids'].to(DEVICE)\n",
    "            attention_mask = ids['attention_mask'].to(DEVICE)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                embedding_repr = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "                #emb = embedding_repr.last_hidden_state[0,:len(record.seq)].cpu().numpy()\n",
    "                \n",
    "                hidden = embedding_repr.last_hidden_state[0]   # shape: [L+2, 2560]\n",
    "                emb = hidden[1:-1].cpu().numpy()              # remove BOS/EOS\n",
    "                \n",
    "                np.save(out_path+f'{record.id}.npy',emb)\n",
    "                \n",
    "                \n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#Final file count of 32332\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
