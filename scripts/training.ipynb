{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is an example of reproducing training results\n",
    "Prior to this step, please execute `download_weights.py` to download the weights for all pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T22:51:57.301644Z",
     "start_time": "2025-12-05T22:51:55.973816Z"
    }
   },
   "source": [
    "# import packages\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # <-- add this at the top\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import readData\n",
    "import shutil\n",
    "from utils import *\n",
    "import torch\n",
    "from torch import nn\n",
    "from model import LABind\n",
    "from func_help import setALlSeed,get_std_opt\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "from sklearn.model_selection import KFold\n",
    "import gc"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T22:52:14.476879Z",
     "start_time": "2025-12-05T22:52:14.460944Z"
    }
   },
   "source": [
    "# config\n",
    "DEVICE = torch.device('cuda:0')\n",
    "root_path = getRootPath()\n",
    "dataset = 'LigBind' # DS1:LigBind, DS2:GPSite, DS3:Unseen\n",
    "\n",
    "nn_config = {\n",
    "    # dataset \n",
    "    'train_file': f'{root_path}/{dataset}/label/train/train.fa',\n",
    "    'test_file': f'{root_path}/{dataset}/label/test/test.fa',\n",
    "    'valid_file': f'{root_path}/{dataset}/label/picking.fa',\n",
    "    'proj_dir': f'{root_path}/{dataset}',\n",
    "    'lig_dict': pkl.load(open(f'{root_path}/tools/ligand.pkl', 'rb')),\n",
    "    'pdb_class':'source', # source or omegafold or esmfold\n",
    "    'dssp_max_repr': np.load(f'{root_path}/tools/dssp_max_repr.npy'),\n",
    "    'dssp_min_repr': np.load(f'{root_path}/tools/dssp_min_repr.npy'),\n",
    "    'ankh_max_repr': np.load(f'{root_path}/tools/ankh_max_repr.npy'),\n",
    "    'ankh_min_repr': np.load(f'{root_path}/tools/ankh_min_repr.npy'),\n",
    "    'esm2_max_repr': np.load(f'{root_path}/tools/esm2_max_repr.npy'),\n",
    "    'esm2_min_repr': np.load(f'{root_path}/tools/esm2_min_repr.npy'),\n",
    "    'ion_max_repr': np.load(f'{root_path}/tools/ion_max_repr.npy'),\n",
    "    'ion_min_repr': np.load(f'{root_path}/tools/ion_min_repr.npy'),\n",
    "    # model parameters\n",
    "    \n",
    "    'rfeat_dim':2580,\n",
    "    'ligand_dim':768, \n",
    "    'hidden_dim':256, \n",
    "    'heads':4, \n",
    "    'augment_eps':0.05, \n",
    "    'rbf_num':8, \n",
    "    'top_k':30, \n",
    "    'attn_drop':0.1, \n",
    "    'dropout':0.1, \n",
    "    'num_layers':4, \n",
    "    'lr':0.00002, \n",
    "    \n",
    "    # training parameters \n",
    "    # You can modify it according to the actual situation. \n",
    "    # Since it involves mapping the entire protein, it will consume a large amount of GPU memory.\n",
    "    'batch_size':1,\n",
    "    'max_patience':10,\n",
    "    'device_ids':[0]\n",
    "}\n",
    "pretrain_path = { # Please modify \n",
    "    'esmfold_path': '../tools/esmfold_v1', # esmfold path\n",
    "    'esm2_path': '../tools/esm2', \n",
    "    'ankh_path': '../tools/ankh-large/', # ankh path\n",
    "    'molformer_path': '../tools/MoLFormer-XL-both-10pct/', # molformer path\n",
    "    'model_path':f'{root_path}/model/LigBind/' # based on Unseen\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T22:52:17.903954Z",
     "start_time": "2025-12-05T22:52:17.901584Z"
    }
   },
   "source": [
    "print(nn_config.keys())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train_file', 'test_file', 'valid_file', 'proj_dir', 'lig_dict', 'pdb_class', 'dssp_max_repr', 'dssp_min_repr', 'ankh_max_repr', 'ankh_min_repr', 'esm2_max_repr', 'esm2_min_repr', 'ion_max_repr', 'ion_min_repr', 'rfeat_dim', 'ligand_dim', 'hidden_dim', 'heads', 'augment_eps', 'rbf_num', 'top_k', 'attn_drop', 'dropout', 'num_layers', 'lr', 'batch_size', 'max_patience', 'device_ids'])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset\n",
    "Download the file from https://zenodo.org/records/13938443 and place it in the root directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### ESM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00,  2.04it/s]\n",
      "Some weights of EsmModel were not initialized from the model checkpoint at ../tools/esm2 and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda\n",
      "\n",
      "Processing CO3.fa (21 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 21/21 [00:00<00:00, 106120.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing SO4.fa (31 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 31/31 [00:00<00:00, 132948.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing MG.fa (665 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|███████████████████████████████▌      | 553/665 [00:00<00:00, 26351.10it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacty of 15.69 GiB of which 10.12 MiB is free. Including non-PyTorch memory, this process has 15.49 GiB memory in use. Of the allocated memory 15.14 GiB is allocated by PyTorch, and 79.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 65\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;66;03m# Run model on GPU\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m---> 65\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mencoded\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m     hidden \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mlast_hidden_state\u001B[38;5;241m.\u001B[39mcpu()      \u001B[38;5;66;03m# shape: (1, L, D)\u001B[39;00m\n\u001B[1;32m     67\u001B[0m     embedding \u001B[38;5;241m=\u001B[39m hidden[\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m:\u001B[38;5;28mlen\u001B[39m(seq)\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mnumpy()\n",
      "File \u001B[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/transformers/models/esm/modeling_esm.py:914\u001B[0m, in \u001B[0;36mEsmModel.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    905\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[1;32m    907\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[1;32m    908\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m    909\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    912\u001B[0m     past_key_values_length\u001B[38;5;241m=\u001B[39mpast_key_values_length,\n\u001B[1;32m    913\u001B[0m )\n\u001B[0;32m--> 914\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    915\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    916\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    917\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    918\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    919\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    920\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    921\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    922\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    923\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    924\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    925\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    926\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    927\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/transformers/models/esm/modeling_esm.py:619\u001B[0m, in \u001B[0;36mEsmEncoder.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    608\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[1;32m    609\u001B[0m         layer_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[1;32m    610\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    616\u001B[0m         output_attentions,\n\u001B[1;32m    617\u001B[0m     )\n\u001B[1;32m    618\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 619\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    620\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    621\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    622\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    623\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    624\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    625\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    626\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    627\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    629\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/transformers/models/esm/modeling_esm.py:509\u001B[0m, in \u001B[0;36mEsmLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    497\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    498\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    499\u001B[0m     hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    506\u001B[0m ):\n\u001B[1;32m    507\u001B[0m     \u001B[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001B[39;00m\n\u001B[1;32m    508\u001B[0m     self_attn_past_key_value \u001B[38;5;241m=\u001B[39m past_key_value[:\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 509\u001B[0m     self_attention_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    510\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    511\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    512\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    513\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    514\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mself_attn_past_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    515\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    516\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m self_attention_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    518\u001B[0m     \u001B[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001B[39;00m\n",
      "File \u001B[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/transformers/models/esm/modeling_esm.py:443\u001B[0m, in \u001B[0;36mEsmAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    432\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    433\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    434\u001B[0m     hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    440\u001B[0m     output_attentions\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    441\u001B[0m ):\n\u001B[1;32m    442\u001B[0m     hidden_states_ln \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mLayerNorm(hidden_states)\n\u001B[0;32m--> 443\u001B[0m     self_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    444\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states_ln\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    445\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    446\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    447\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    448\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    449\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    450\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    451\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    452\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(self_outputs[\u001B[38;5;241m0\u001B[39m], hidden_states)\n\u001B[1;32m    453\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (attention_output,) \u001B[38;5;241m+\u001B[39m self_outputs[\u001B[38;5;241m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n",
      "File \u001B[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/transformers/models/esm/modeling_esm.py:344\u001B[0m, in \u001B[0;36mEsmSelfAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    341\u001B[0m     past_key_value \u001B[38;5;241m=\u001B[39m (key_layer, value_layer)\n\u001B[1;32m    343\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposition_embedding_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrotary\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 344\u001B[0m     query_layer, key_layer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrotary_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_layer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey_layer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;66;03m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001B[39;00m\n\u001B[1;32m    347\u001B[0m attention_scores \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmatmul(query_layer, key_layer\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m))\n",
      "File \u001B[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/transformers/models/esm/modeling_esm.py:125\u001B[0m, in \u001B[0;36mRotaryEmbedding.forward\u001B[0;34m(self, q, k)\u001B[0m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, q: torch\u001B[38;5;241m.\u001B[39mTensor, k: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor, torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[1;32m    122\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cos_cached, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sin_cached \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_cos_sin_tables(k, seq_dimension\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m--> 125\u001B[0m         \u001B[43mapply_rotary_pos_emb\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cos_cached\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sin_cached\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    126\u001B[0m         apply_rotary_pos_emb(k, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cos_cached, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sin_cached),\n\u001B[1;32m    127\u001B[0m     )\n",
      "File \u001B[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/transformers/models/esm/modeling_esm.py:60\u001B[0m, in \u001B[0;36mapply_rotary_pos_emb\u001B[0;34m(x, cos, sin)\u001B[0m\n\u001B[1;32m     57\u001B[0m cos \u001B[38;5;241m=\u001B[39m cos[:, :, : x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m], :]\n\u001B[1;32m     58\u001B[0m sin \u001B[38;5;241m=\u001B[39m sin[:, :, : x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m], :]\n\u001B[0;32m---> 60\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mcos\u001B[49m) \u001B[38;5;241m+\u001B[39m (rotate_half(x) \u001B[38;5;241m*\u001B[39m sin)\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacty of 15.69 GiB of which 10.12 MiB is free. Including non-PyTorch memory, this process has 15.49 GiB memory in use. Of the allocated memory 15.14 GiB is allocated by PyTorch, and 79.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from Bio import SeqIO\n",
    "from transformers import AutoTokenizer, AutoModel \n",
    "from Bio import SeqIO\n",
    "# 1️⃣ Delete all tensors and models\n",
    "del model\n",
    "gc.collect()\n",
    "\n",
    "# 2️⃣ Empty PyTorch cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 3️⃣ Optional: reset CUDA memory stats\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "MODEL_DIR = \"../tools/esm2\"   # <-- directory containing HF files\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 4\n",
    "FASTA_ROOT = \"fasta\"             # root directory containing subdirs of FASTA files\n",
    "SAVE_ROOT = \"embeddings\"         # root output folder\n",
    "out_path = f\"{root_path}/{dataset}/esm/\"\n",
    "fasta_path = f\"{root_path}/{dataset}/fasta/\"\n",
    "\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrain_path['esm2_path'])\n",
    "model     = AutoModel.from_pretrained(pretrain_path['esm2_path'])\n",
    "model.to(DEVICE)\n",
    "\n",
    "model.eval()\n",
    "model.gradient_checkpointing_enable()\n",
    "print(\"Model loaded on\", DEVICE)\n",
    "for file_class in os.listdir(fasta_path):\n",
    "    class_path = os.path.join(fasta_path, file_class)\n",
    "\n",
    "    for fasta_file in os.listdir(class_path):\n",
    "        fasta_path_full = os.path.join(class_path, fasta_file)\n",
    "\n",
    "        sequences = list(SeqIO.parse(fasta_path_full, \"fasta\"))\n",
    "\n",
    "        print(f\"\\nProcessing {fasta_file} ({len(sequences)} sequences)\")\n",
    "\n",
    "        for record in tqdm(sequences):\n",
    "            save_path = os.path.join(out_path, f\"{record.id}.npy\")\n",
    "\n",
    "            # Skip if already processed\n",
    "            if os.path.exists(save_path):\n",
    "                continue\n",
    "\n",
    "            seq = str(record.seq)\n",
    "\n",
    "            # Tokenize for HF model\n",
    "            encoded = tokenizer(\n",
    "                seq,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=False,\n",
    "                add_special_tokens=True\n",
    "            )\n",
    "\n",
    "            encoded = {k: v.to(DEVICE) for k, v in encoded.items()}\n",
    "\n",
    "            # Run model on GPU\n",
    "            with torch.no_grad():\n",
    "                output = model(**encoded)\n",
    "                hidden = output.last_hidden_state.cpu()      # shape: (1, L, D)\n",
    "                embedding = hidden[0, 1:len(seq)+1].numpy()\n",
    "\n",
    "            # Save\n",
    "            np.save(save_path, embedding)\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        \n",
    "\n",
    "# Cleanup\n",
    "del model\n",
    "gc.collect()\n",
    "print(\"\\n✓ Done extracting ESM embeddings!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping bad file: 6s9fB.npy Error: Failed to interpret file '/virtual/zengzix4/LABind_ESM/LigBind/esm3B/6s9fB.npy' as a pickle\n",
      "Skipping bad file: 6ptkA.npy Error: cannot reshape array of size 1138656 into shape (471,2560)\n",
      "FINAL ESM MIN SHAPE: (2560,)\n",
      "FINAL ESM MAX SHAPE: (2560,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "esm_dir = \"/virtual/zengzix4/LABind_ESM/LigBind/esm3B\"    # <-- CHANGE THIS\n",
    "save_file = \"esm2_repr_stats.npy\"\n",
    "save_dir = \"/virtual/zengzix4/LABind_ESM/tools/\"\n",
    "# Use None so we can initialize on first file\n",
    "\n",
    "all_min = None\n",
    "all_max = None\n",
    "\n",
    "for f in os.listdir(esm_dir):\n",
    "    if not f.endswith(\".npy\"):\n",
    "        continue\n",
    "\n",
    "    fpath = os.path.join(esm_dir, f)\n",
    "\n",
    "    try:\n",
    "        arr = np.load(fpath, allow_pickle=True)\n",
    "    except Exception as e:\n",
    "        print(\"Skipping bad file:\", f, \"Error:\", e)\n",
    "        continue\n",
    "\n",
    "    # Convert object arrays to real numpy if possible\n",
    "    if isinstance(arr, np.ndarray) and arr.dtype == object:\n",
    "        try:\n",
    "            arr = np.vstack(arr)  # many ESM embeddings come as list of arrays\n",
    "        except:\n",
    "            print(\"Skipping malformed object array:\", f)\n",
    "            continue\n",
    "\n",
    "    # Now check shape\n",
    "    if arr.ndim != 2:\n",
    "        print(\"Skipping wrong shape\", f, arr.shape)\n",
    "        continue\n",
    "\n",
    "    # Compute per-dimension min/max\n",
    "    fmin = arr.min(axis=0)\n",
    "    fmax = arr.max(axis=0)\n",
    "\n",
    "    if all_min is None:\n",
    "        all_min = fmin\n",
    "        all_max = fmax\n",
    "    else:\n",
    "        all_min = np.minimum(all_min, fmin)\n",
    "        all_max = np.maximum(all_max, fmax)\n",
    "\n",
    "print(\"FINAL ESM MIN SHAPE:\", all_min.shape)\n",
    "print(\"FINAL ESM MAX SHAPE:\", all_max.shape)\n",
    "np.save(os.path.join(save_dir, \"esm2_min_repr.npy\"), all_min)\n",
    "np.save(os.path.join(save_dir, \"esm2_max_repr.npy\"), all_max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Ankh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/virtual/zengzix4/miniconda/envs/LABind/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ankh features\n",
    "from transformers import AutoTokenizer, T5EncoderModel \n",
    "from Bio import SeqIO\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrain_path['ankh_path'])\n",
    "model     = T5EncoderModel.from_pretrained(pretrain_path['ankh_path'])\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "out_path = f\"{root_path}/{dataset}/ankh/\"\n",
    "makeDir(out_path)\n",
    "# 使用biopython读取fasta文件\n",
    "fasta_path = f\"{root_path}/{dataset}/fasta/\"\n",
    "for file_class in os.listdir(fasta_path):\n",
    "    for fasta_base_file in os.listdir(f\"{fasta_path}/{file_class}\"):\n",
    "        fasta_file = f\"{fasta_path}/{file_class}/{fasta_base_file}\"\n",
    "        sequences = SeqIO.parse(fasta_file, \"fasta\")\n",
    "        for record in tqdm(sequences):\n",
    "            if os.path.exists(out_path+f'{record.id}.npy'):\n",
    "                continue\n",
    "            ids = tokenizer.batch_encode_plus([list(record.seq)], add_special_tokens=True, padding=True, is_split_into_words=True, return_tensors=\"pt\")\n",
    "            input_ids = ids['input_ids'].to(DEVICE)\n",
    "            attention_mask = ids['attention_mask'].to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                embedding_repr = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "                emb = embedding_repr.last_hidden_state[0,:len(record.seq)].cpu().numpy()\n",
    "                np.save(out_path+f'{record.id}.npy',emb)\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DSSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DSSP running: 100%|███████████████| 11121/11121 [1:22:02<00:00,  2.26proteins/s]\n"
     ]
    }
   ],
   "source": [
    "from Bio.PDB import PDBParser\n",
    "from Bio.PDB.DSSP import DSSP\n",
    "\n",
    "mapSS = {' ':[0,0,0,0,0,0,0,0,0],\n",
    "        '-':[1,0,0,0,0,0,0,0,0],\n",
    "        'H':[0,1,0,0,0,0,0,0,0],\n",
    "        'B':[0,0,1,0,0,0,0,0,0],\n",
    "        'E':[0,0,0,1,0,0,0,0,0],\n",
    "        'G':[0,0,0,0,1,0,0,0,0],\n",
    "        'I':[0,0,0,0,0,1,0,0,0],\n",
    "        'P':[0,0,0,0,0,0,1,0,0],\n",
    "        'T':[0,0,0,0,0,0,0,1,0],\n",
    "        'S':[0,0,0,0,0,0,0,0,1]}\n",
    "p = PDBParser(QUIET=True)\n",
    "pdb_path = f\"{root_path}/{dataset}/pdb/\"\n",
    "dssp_path = \"../tools/mkdssp\"\n",
    "pdb_class = nn_config['pdb_class']\n",
    "makeDir(f\"{root_path}/{dataset}/{pdb_class}_dssp/\")\n",
    "test_files = os.listdir(pdb_path)\n",
    "pdb_files = []\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(pdb_path):\n",
    "    for f in filenames:\n",
    "        if f.endswith('.pdb'):\n",
    "            pdb_files.append(os.path.join(dirpath, f)) \n",
    "            \n",
    "for pdb_file_name in tqdm(pdb_files, desc='DSSP running',ncols=80,unit='proteins'):\n",
    "    pdb_file = pdb_file_name\n",
    "    save_file = pdb_file.replace('.pdb','.npy').replace('pdb',f'{pdb_class}_dssp')\n",
    "    if os.path.exists(save_file):\n",
    "        continue\n",
    "    structure = p.get_structure(\"tmp\", pdb_file)\n",
    "    model = structure[0]\n",
    "    try:\n",
    "        dssp = DSSP(model, pdb_file, dssp=dssp_path)\n",
    "        keys = list(dssp.keys())\n",
    "    except:\n",
    "        keys = []\n",
    "    res_np = []\n",
    "    for chain in model:\n",
    "        for residue in chain:\n",
    "            res_key = (chain.id,(' ', residue.id[1], residue.id[2]))\n",
    "            if res_key in keys:\n",
    "                tuple_dssp = dssp[res_key]\n",
    "                res_np.append(mapSS[tuple_dssp[2]] + list(tuple_dssp[3:]))\n",
    "            else:\n",
    "                res_np.append(np.zeros(20))\n",
    "    os.makedirs(os.path.dirname(save_file), exist_ok=True)\n",
    "    np.save(save_file, np.array(res_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB.ResidueDepth import get_surface\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "pdb_path = f\"{root_path}/{dataset}/pdb/\"\n",
    "msms_path = \"../tools/msms\"\n",
    "pdb_class = nn_config['pdb_class']\n",
    "makeDir(f\"{root_path}/{dataset}/{pdb_class}_pos/\")\n",
    "pdb_files = []\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(pdb_path):\n",
    "    for f in filenames:\n",
    "        if f.endswith('.pdb'):\n",
    "            pdb_files.append(os.path.join(dirpath, f)) \n",
    "\n",
    "for pdb_file in tqdm(pdb_files,desc='MSMS running',ncols=80,unit='proteins'):\n",
    "    save_file = pdb_file.replace('.pdb','.npy').replace('pdb',f'{pdb_class}_pos')\n",
    "    if os.path.exists(save_file):\n",
    "        continue\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    X = []\n",
    "    chain_atom = ['N', 'CA', 'C', 'O']\n",
    "    model = parser.get_structure('model', pdb_file)[0]\n",
    "    chain = next(model.get_chains())\n",
    "    try:\n",
    "        surf = get_surface(chain,MSMS=msms_path)\n",
    "        surf_tree = cKDTree(surf)\n",
    "    except:\n",
    "        surf = np.empty(0)\n",
    "    for residue in chain:\n",
    "        line = []\n",
    "        atoms_coord = np.array([atom.get_coord() for atom in residue])\n",
    "        if surf.size == 0:\n",
    "            dist, _ = surf_tree.query(atoms_coord)\n",
    "            closest_atom = np.argmin(dist)\n",
    "            closest_pos = atoms_coord[closest_atom]\n",
    "        else:\n",
    "            closest_pos = atoms_coord[-1]\n",
    "        atoms = list(residue.get_atoms())\n",
    "        try:\n",
    "            ca_pos = residue['CA'].get_coord()\n",
    "        except KeyError:\n",
    "            print(residue)\n",
    "            continue\n",
    "\n",
    "        pos_s = 0\n",
    "        un_s = 0\n",
    "        for atom in atoms:\n",
    "            if atom.name in chain_atom:\n",
    "                line.append(atom.get_coord())\n",
    "            else:\n",
    "                pos_s += calMass(atom,True)\n",
    "                un_s += calMass(atom,False)\n",
    "        # 此处line应该等于4\n",
    "        if len(line) != 4:\n",
    "            line = line + [list(ca_pos)]*(4-len(line))\n",
    "        if un_s == 0:\n",
    "            R_pos = ca_pos\n",
    "        else:\n",
    "            R_pos = pos_s / un_s\n",
    "        line.append(R_pos)  \n",
    "        line.append(closest_pos) # 加入最近点的残基信息\n",
    "        X.append(line) \n",
    "    \n",
    "    os.makedirs(os.path.dirname(save_file), exist_ok=True)\n",
    "\n",
    "    np.save(save_file, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T22:52:27.996922Z",
     "start_time": "2025-12-05T22:52:27.989368Z"
    }
   },
   "source": [
    "def valid(model, valid_list, fold_idx):\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    valid_data = readData.readData(\n",
    "        name_list=valid_list, \n",
    "        proj_dir=nn_config['proj_dir'], \n",
    "        lig_dict=nn_config['lig_dict'],\n",
    "        true_file=nn_config['train_file'], mode='train') # If 5-fold cross-validation is not used, it needs to be changed to valid_file.\n",
    "    valid_loader = DataLoader(valid_data, batch_size=nn_config['batch_size'],shuffle=True, collate_fn=valid_data.collate_fn, num_workers=5)\n",
    "    all_y_score = []\n",
    "    all_y_true = []\n",
    "    with torch.no_grad():\n",
    "        for rfeat, ligand, xyz,  mask, y_true in valid_loader:\n",
    "            tensors = [rfeat, ligand, xyz,  mask, y_true]\n",
    "            tensors = [tensor.to(DEVICE) for tensor in tensors]\n",
    "            rfeat, ligand, xyz, mask, y_true = tensors\n",
    "            logits = model(rfeat, ligand, xyz,  mask).sigmoid() # [N]\n",
    "            logits = torch.masked_select(logits, mask==1)\n",
    "            y_true = torch.masked_select(y_true, mask==1)\n",
    "            all_y_score.extend(logits.cpu().detach().numpy())\n",
    "            all_y_true.extend(y_true.cpu().detach().numpy())\n",
    "        # 通过aupr数值进行早停\n",
    "        aupr_value = average_precision_score(all_y_true, all_y_score)\n",
    "    return aupr_value\n",
    "\n",
    "def train(train_list,valid_list=None,model=None,epochs=50,fold_idx=None):\n",
    "    model.to(DEVICE)\n",
    "    train_data = readData.readData(\n",
    "        name_list=train_list, \n",
    "        proj_dir=nn_config['proj_dir'], \n",
    "        lig_dict=nn_config['lig_dict'],\n",
    "        true_file=nn_config['train_file'], mode='train')\n",
    "    train_loader = DataLoader(train_data, batch_size=nn_config['batch_size'],shuffle=True, collate_fn=train_data.collate_fn, num_workers=5)\n",
    "    loss_fn = nn.BCELoss(reduction='none')\n",
    "    optimizer = get_std_opt(len(train_list),nn_config['batch_size'], model.parameters(), nn_config['hidden_dim'], nn_config['lr'])\n",
    "    v_max_aupr = 0\n",
    "    patience = 0\n",
    "    t_mccs = []\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model,device_ids=nn_config['device_ids'])\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        all_loss = 0\n",
    "        all_cnt = 0\n",
    "        model.train()\n",
    "        for rfeat, ligand, xyz,  mask, y_true in tqdm(train_loader):\n",
    "            tensors = [rfeat, ligand, xyz,  mask, y_true]\n",
    "            tensors = [tensor.to(DEVICE) for tensor in tensors]\n",
    "            rfeat, ligand, xyz, mask, y_true = tensors\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(rfeat, ligand, xyz, mask).sigmoid() # [N]\n",
    "            # 计算所有离子的loss\n",
    "            loss = loss_fn(logits, y_true) * mask\n",
    "            loss = loss.sum() / mask.sum()\n",
    "            all_loss += loss.item()\n",
    "            all_cnt += 1\n",
    "            loss.backward()\n",
    "            # NEW\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        train_losses.append(all_loss / all_cnt)\n",
    "        # 根据验证集的aupr进行早停\n",
    "        if valid_list is not None:\n",
    "            v_aupr = valid(model,valid_list, fold_idx)\n",
    "            t_mccs.append(v_aupr)\n",
    "            print(f'Epoch {epoch} Loss: {all_loss / all_cnt}', f'Epoch Valid {epoch} AUPR: {v_aupr}')\n",
    "            if v_aupr > v_max_aupr:\n",
    "                v_max_aupr = v_aupr\n",
    "                patience = 0\n",
    "                torch.save(model.state_dict(), f'{root_path}/Output/{dataset}_5fold/fold{fold_idx}.ckpt')\n",
    "            else:\n",
    "                patience += 1\n",
    "            if patience >= nn_config['max_patience']:\n",
    "                break\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setALlSeed(11)\n",
    "# 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state = 42)\n",
    "data_list = readDataList(f'{root_path}/{dataset}/label/train/train.fa',skew=1)\n",
    "makeDir(f'{root_path}/Output/{dataset}_5fold/')\n",
    "fold_idx = 0\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "for train_idx, valid_idx in kf.split(data_list):\n",
    "    train_list = [data_list[i] for i in train_idx]\n",
    "    valid_list = [data_list[j] for j in valid_idx]\n",
    "    model = LABind(\n",
    "    rfeat_dim=nn_config['rfeat_dim'], ligand_dim=nn_config['ligand_dim'], hidden_dim=nn_config['hidden_dim'], heads=nn_config['heads'], augment_eps=nn_config['augment_eps'], \n",
    "    rbf_num=nn_config['rbf_num'],top_k=nn_config['top_k'], attn_drop=nn_config['attn_drop'], dropout=nn_config['dropout'], num_layers=nn_config['num_layers'])\n",
    "    train(train_list,valid_list,model,epochs=70,fold_idx=fold_idx)\n",
    "    fold_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T02:20:04.831822Z",
     "start_time": "2025-12-05T22:52:48.341878Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "all_data = readDataList(f'{root_path}/{dataset}/label/train/train.fa', skew=1) \n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "train_list, valid_list = train_test_split( ## no given validation in LigBind\n",
    "    all_data,\n",
    "    test_size=0.1,       # 10% validation\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "makeDir(f'{root_path}/Output/{dataset}/')\n",
    "model = LABind(\n",
    "rfeat_dim=nn_config['rfeat_dim'], ligand_dim=nn_config['ligand_dim'], hidden_dim=nn_config['hidden_dim'], heads=nn_config['heads'], augment_eps=nn_config['augment_eps'], \n",
    "rbf_num=nn_config['rbf_num'],top_k=nn_config['top_k'], attn_drop=nn_config['attn_drop'], dropout=nn_config['dropout'], num_layers=nn_config['num_layers'])\n",
    "train(train_list, valid_list, model, epochs=30, fold_idx=0)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7452/7452 [06:34<00:00, 18.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 0.16773487191539313 Epoch Valid 0 AUPR: 0.15188207743819437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7452/7452 [06:34<00:00, 18.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.09301851297996416 Epoch Valid 1 AUPR: 0.3303212493664102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7452/7452 [06:36<00:00, 18.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.08294687472638522 Epoch Valid 2 AUPR: 0.3539388712708303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7452/7452 [06:32<00:00, 18.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.07778382939987022 Epoch Valid 3 AUPR: 0.37415072731590243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7452/7452 [06:31<00:00, 19.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.07421066779255403 Epoch Valid 4 AUPR: 0.4264595256164272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7452/7452 [06:31<00:00, 19.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 0.07090300756219614 Epoch Valid 5 AUPR: 0.4942124884467164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7452/7452 [06:32<00:00, 18.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 0.06766264808522042 Epoch Valid 6 AUPR: 0.5331649356952992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7452/7452 [06:33<00:00, 18.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: 0.06505626051700912 Epoch Valid 7 AUPR: 0.5362209718284536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7452/7452 [06:34<00:00, 18.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss: 0.06308529215774666 Epoch Valid 8 AUPR: 0.5568852732873681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7452/7452 [06:33<00:00, 18.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss: 0.06194183621210203 Epoch Valid 9 AUPR: 0.6071914318149825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7452/7452 [06:36<00:00, 18.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss: 0.06018173068642837 Epoch Valid 10 AUPR: 0.6144897796498618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7452/7452 [06:36<00:00, 18.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Loss: 0.05878642204257871 Epoch Valid 11 AUPR: 0.6075425645010675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7452/7452 [06:35<00:00, 18.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Loss: 0.057485533343340355 Epoch Valid 12 AUPR: 0.6062459300440062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7452/7452 [06:35<00:00, 18.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Loss: 0.055953017686297186 Epoch Valid 13 AUPR: 0.6459185487369488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7452/7452 [06:38<00:00, 18.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Loss: 0.05421930840309267 Epoch Valid 14 AUPR: 0.6268350216191082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7452/7452 [06:37<00:00, 18.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Loss: 0.05325474082991142 Epoch Valid 15 AUPR: 0.6534566494034956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7452/7452 [06:38<00:00, 18.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Loss: 0.05198371539396074 Epoch Valid 16 AUPR: 0.6480825617920363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7452/7452 [06:29<00:00, 19.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Loss: 0.05101577715993653 Epoch Valid 17 AUPR: 0.6743290063081434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7452/7452 [06:33<00:00, 18.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Loss: 0.04996418202401481 Epoch Valid 18 AUPR: 0.6642900747470682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7452/7452 [06:35<00:00, 18.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Loss: 0.049162320829575216 Epoch Valid 19 AUPR: 0.6725385444033093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7452/7452 [06:34<00:00, 18.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Loss: 0.048381847190477875 Epoch Valid 20 AUPR: 0.6710360541261782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7452/7452 [06:34<00:00, 18.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Loss: 0.04776780305565923 Epoch Valid 21 AUPR: 0.6890680764770898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7452/7452 [06:32<00:00, 18.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Loss: 0.04692968144207681 Epoch Valid 22 AUPR: 0.6847713874963732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7452/7452 [06:36<00:00, 18.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Loss: 0.045998911585828514 Epoch Valid 23 AUPR: 0.6812236195925226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7452/7452 [06:32<00:00, 18.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Loss: 0.04551797652682823 Epoch Valid 24 AUPR: 0.6750101550759365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7452/7452 [06:35<00:00, 18.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Loss: 0.04482103356887577 Epoch Valid 25 AUPR: 0.6776572865894952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7452/7452 [06:33<00:00, 18.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Loss: 0.044194285815358995 Epoch Valid 26 AUPR: 0.6852279136186697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7452/7452 [06:33<00:00, 18.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Loss: 0.043578366490061395 Epoch Valid 27 AUPR: 0.6879517398868915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7452/7452 [06:33<00:00, 18.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Loss: 0.042905278586486496 Epoch Valid 28 AUPR: 0.6912391369830506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7452/7452 [06:34<00:00, 18.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Loss: 0.04229644027724296 Epoch Valid 29 AUPR: 0.6902214947664564\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T02:25:04.729959Z",
     "start_time": "2025-12-06T02:24:14.923210Z"
    }
   },
   "source": [
    "# Determine the best threshold for MCC based on the validation set.\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "\n",
    "model_path = f'{root_path}/Output/{dataset}/' # if 5-fold cross-validation, {dataset}_5fold\n",
    "print(model_path)\n",
    "print(nn_config['pdb_class'])\n",
    "\n",
    "models = []\n",
    "for fold in range(1): # if 5-fold cross-validation, set to 5\n",
    "\n",
    "\n",
    "    state_dict = torch.load(model_path + 'fold%s.ckpt'%fold,'cuda:0')\n",
    "    model = LABind(\n",
    "        rfeat_dim=nn_config['rfeat_dim'], ligand_dim=nn_config['ligand_dim'], hidden_dim=nn_config['hidden_dim'], heads=nn_config['heads'], augment_eps=nn_config['augment_eps'], \n",
    "        rbf_num=nn_config['rbf_num'],top_k=nn_config['top_k'], attn_drop=nn_config['attn_drop'], dropout=nn_config['dropout'], num_layers=nn_config['num_layers']).to(DEVICE)\n",
    "    model = nn.DataParallel(model,device_ids=nn_config['device_ids'])\n",
    "    for k, v in state_dict.items():\n",
    "        # if checkpoint lacks \"module.\", add it\n",
    "        if not k.startswith(\"module.\"):\n",
    "            new_state_dict[\"module.\" + k] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "    \n",
    "valid_data = readData.readData(\n",
    "    name_list=valid_list, \n",
    "    proj_dir=nn_config['proj_dir'], \n",
    "    lig_dict=nn_config['lig_dict'],\n",
    "    true_file=f'{root_path}/{dataset}/label/train/train.fa', mode='train')\n",
    "# 打印长度\n",
    "valid_loader = DataLoader(valid_data, batch_size=nn_config['batch_size'], collate_fn=valid_data.collate_fn)\n",
    "print(f'valid data length: {len(valid_data)}')\n",
    "all_y_score = []\n",
    "all_y_true = []\n",
    "with torch.no_grad():\n",
    "    for rfeat, ligand, xyz,  mask, y_true in valid_loader:\n",
    "        tensors = [rfeat, ligand, xyz,  mask, y_true]\n",
    "        tensors = [tensor.to(DEVICE) for tensor in tensors]\n",
    "        rfeat, ligand, xyz, mask, y_true = tensors\n",
    "        \n",
    "        logits = [model(rfeat, ligand, xyz, mask).sigmoid() for model in models]\n",
    "        logits = torch.stack(logits,0).mean(0)\n",
    "        \n",
    "        logits = torch.masked_select(logits, mask==1)\n",
    "        y_true = torch.masked_select(y_true, mask==1)\n",
    "        all_y_score.extend(logits.cpu().detach().numpy())\n",
    "        all_y_true.extend(y_true.cpu().detach().numpy())\n",
    "\n",
    "best_threshold,best_mcc,best_pred = getBestThreshold(all_y_true, all_y_score)\n",
    "appendText(f'{model_path}/Best_Threshold.txt',f'{best_threshold} {best_mcc}\\n')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/virtual/zengzix4/LABind_ESM/Output/LigBind/\n",
      "source\n",
      "valid data length: 828\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T02:49:29.300838Z",
     "start_time": "2025-12-06T02:46:44.669163Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "model_path = f'{root_path}/Output/{dataset}/' # if 5-fold cross-validation, {dataset}_5fold\n",
    "print(model_path)\n",
    "print(nn_config['pdb_class'])\n",
    "\n",
    "models = []\n",
    "for fold in range(1): # if 5-fold cross-validation, set to 5\n",
    "    state_dict = torch.load(model_path + 'fold%s.ckpt'%fold,'cuda:0')\n",
    "    model = LABind(\n",
    "        rfeat_dim=nn_config['rfeat_dim'], ligand_dim=nn_config['ligand_dim'], hidden_dim=nn_config['hidden_dim'], heads=nn_config['heads'], augment_eps=nn_config['augment_eps'], \n",
    "        rbf_num=nn_config['rbf_num'],top_k=nn_config['top_k'], attn_drop=nn_config['attn_drop'], dropout=nn_config['dropout'], num_layers=nn_config['num_layers']).to(DEVICE)\n",
    "    model = nn.DataParallel(model,device_ids=nn_config['device_ids'])\n",
    "    \n",
    "    for k, v in state_dict.items():\n",
    "        # if checkpoint lacks \"module.\", add it\n",
    "        if not k.startswith(\"module.\"):\n",
    "            new_state_dict[\"module.\" + k] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "\n",
    "df = pd.DataFrame(columns=['ligand','Rec','SPE','Acc','Pre','F1','MCC','AUC','AUPR'])\n",
    "for ionic in os.listdir(f'{root_path}/{dataset}/label/test/'):\n",
    "    ionic = ionic.split('.')[0]\n",
    "    test_list = readDataList(f'{root_path}/{dataset}/label/test/{ionic}.fa',skew=1)\n",
    "    test_data = readData.readData(\n",
    "        name_list=test_list, \n",
    "        proj_dir=nn_config['proj_dir'], \n",
    "        lig_dict=nn_config['lig_dict'],\n",
    "        true_file=f'{root_path}/{dataset}/label/test/{ionic}.fa', mode='test')\n",
    "    # 打印长度\n",
    "    test_loader = DataLoader(test_data, batch_size=nn_config['batch_size'], collate_fn=test_data.collate_fn)\n",
    "    print(f'{ionic} test data length: {len(test_data)}')\n",
    "    all_y_score = []\n",
    "    all_y_true = []\n",
    "    with torch.no_grad():\n",
    "        for rfeat, ligand, xyz,  mask, y_true in test_loader:\n",
    "            if rfeat == None:\n",
    "                continue\n",
    "            tensors = [rfeat, ligand, xyz,  mask, y_true]\n",
    "            tensors = [tensor.to(DEVICE) for tensor in tensors]\n",
    "            rfeat, ligand, xyz, mask, y_true = tensors\n",
    "            \n",
    "            logits = [model(rfeat, ligand, xyz, mask).sigmoid() for model in models]\n",
    "            logits = torch.stack(logits,0).mean(0)\n",
    "            \n",
    "            logits = torch.masked_select(logits, mask==1)\n",
    "            y_true = torch.masked_select(y_true, mask==1)\n",
    "            all_y_score.extend(logits.cpu().detach().numpy())\n",
    "            all_y_true.extend(y_true.cpu().detach().numpy())\n",
    "    data_dict = calEval(all_y_true, all_y_score) # please set best threshold.\n",
    "    data_dict['ligand'] = ionic\n",
    "    df = pd.concat([df,pd.DataFrame(data_dict,index=[0])])\n",
    "df.to_csv(f'{model_path}test.csv',index=False)\n",
    "df"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/virtual/zengzix4/LABind_ESM/Output/LigBind/\n",
      "source\n",
      "CO3 test data length: 21\n",
      "SO4 test data length: 29\n",
      "MG test data length: 665\n",
      "CU test data length: 54\n",
      "HEM test data length: 78\n",
      "ATP test data length: 139\n",
      "FE2 test data length: 31\n",
      "K test data length: 17\n",
      "NA test data length: 23\n",
      "MN test data length: 167\n",
      "ADP test data length: 158\n",
      "ZN test data length: 615\n",
      "PO4 test data length: 73\n",
      "FE test data length: 88\n",
      "test test data length: 2839\n",
      "CA test data length: 533\n",
      "AMP test data length: 67\n",
      "NO2 test data length: 7\n",
      "GTP test data length: 34\n",
      "GDP test data length: 40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  ligand       Rec       SPE       Acc       Pre        F1       MCC  \\\n",
       "0    CO3  0.292929  0.992662  0.984684  0.315217  0.303665  0.296136   \n",
       "0    SO4  0.474654  0.983408  0.972413  0.387218  0.426501  0.414762   \n",
       "0     MG  0.353815  0.996990  0.991491  0.503429  0.415566  0.417918   \n",
       "0     CU  0.796680  0.994128  0.991259  0.666667  0.725898  0.724450   \n",
       "0    HEM  0.899337  0.942007  0.939122  0.529266  0.666369  0.662440   \n",
       "0    ATP  0.791045  0.974459  0.969642  0.455138  0.577820  0.586522   \n",
       "0    FE2  0.861789  0.996896  0.995198  0.779412  0.818533  0.817164   \n",
       "0      K  0.496599  0.972991  0.963580  0.270370  0.350120  0.349453   \n",
       "0     NA  0.427136  0.974791  0.960483  0.312500  0.360934  0.345491   \n",
       "0     MN  0.712082  0.995547  0.992119  0.661888  0.686068  0.682550   \n",
       "0    ADP  0.880189  0.973365  0.971172  0.443356  0.589685  0.613110   \n",
       "0     ZN  0.853968  0.996187  0.994112  0.768352  0.808901  0.807079   \n",
       "0    PO4  0.553942  0.987501  0.979681  0.448739  0.495822  0.488376   \n",
       "0     FE  0.817391  0.995613  0.993608  0.679518  0.742105  0.742131   \n",
       "0   test  0.698784  0.989819  0.985000  0.536088  0.606718  0.604683   \n",
       "0     CA  0.583655  0.994317  0.987880  0.620537  0.601531  0.595672   \n",
       "0    AMP  0.688295  0.962877  0.954786  0.360186  0.472902  0.477708   \n",
       "0    NO2  0.116279  0.985535  0.962645  0.178571  0.140845  0.125581   \n",
       "0    GTP  0.748899  0.971088  0.964416  0.445026  0.558292  0.560977   \n",
       "0    GDP  0.816532  0.974398  0.969564  0.501859  0.621642  0.626306   \n",
       "\n",
       "        AUC      AUPR  \n",
       "0  0.848082  0.255461  \n",
       "0  0.888873  0.403882  \n",
       "0  0.904224  0.351722  \n",
       "0  0.988084  0.796941  \n",
       "0  0.974140  0.774099  \n",
       "0  0.972563  0.663120  \n",
       "0  0.998369  0.901799  \n",
       "0  0.920470  0.357842  \n",
       "0  0.802306  0.300137  \n",
       "0  0.961264  0.721794  \n",
       "0  0.982973  0.721988  \n",
       "0  0.989792  0.879585  \n",
       "0  0.944524  0.469653  \n",
       "0  0.991364  0.822914  \n",
       "0  0.961379  0.664219  \n",
       "0  0.930920  0.604939  \n",
       "0  0.942346  0.462122  \n",
       "0  0.745298  0.136813  \n",
       "0  0.962145  0.647657  \n",
       "0  0.976589  0.765284  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ligand</th>\n",
       "      <th>Rec</th>\n",
       "      <th>SPE</th>\n",
       "      <th>Acc</th>\n",
       "      <th>Pre</th>\n",
       "      <th>F1</th>\n",
       "      <th>MCC</th>\n",
       "      <th>AUC</th>\n",
       "      <th>AUPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CO3</td>\n",
       "      <td>0.292929</td>\n",
       "      <td>0.992662</td>\n",
       "      <td>0.984684</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>0.303665</td>\n",
       "      <td>0.296136</td>\n",
       "      <td>0.848082</td>\n",
       "      <td>0.255461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SO4</td>\n",
       "      <td>0.474654</td>\n",
       "      <td>0.983408</td>\n",
       "      <td>0.972413</td>\n",
       "      <td>0.387218</td>\n",
       "      <td>0.426501</td>\n",
       "      <td>0.414762</td>\n",
       "      <td>0.888873</td>\n",
       "      <td>0.403882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MG</td>\n",
       "      <td>0.353815</td>\n",
       "      <td>0.996990</td>\n",
       "      <td>0.991491</td>\n",
       "      <td>0.503429</td>\n",
       "      <td>0.415566</td>\n",
       "      <td>0.417918</td>\n",
       "      <td>0.904224</td>\n",
       "      <td>0.351722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CU</td>\n",
       "      <td>0.796680</td>\n",
       "      <td>0.994128</td>\n",
       "      <td>0.991259</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.725898</td>\n",
       "      <td>0.724450</td>\n",
       "      <td>0.988084</td>\n",
       "      <td>0.796941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HEM</td>\n",
       "      <td>0.899337</td>\n",
       "      <td>0.942007</td>\n",
       "      <td>0.939122</td>\n",
       "      <td>0.529266</td>\n",
       "      <td>0.666369</td>\n",
       "      <td>0.662440</td>\n",
       "      <td>0.974140</td>\n",
       "      <td>0.774099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATP</td>\n",
       "      <td>0.791045</td>\n",
       "      <td>0.974459</td>\n",
       "      <td>0.969642</td>\n",
       "      <td>0.455138</td>\n",
       "      <td>0.577820</td>\n",
       "      <td>0.586522</td>\n",
       "      <td>0.972563</td>\n",
       "      <td>0.663120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FE2</td>\n",
       "      <td>0.861789</td>\n",
       "      <td>0.996896</td>\n",
       "      <td>0.995198</td>\n",
       "      <td>0.779412</td>\n",
       "      <td>0.818533</td>\n",
       "      <td>0.817164</td>\n",
       "      <td>0.998369</td>\n",
       "      <td>0.901799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K</td>\n",
       "      <td>0.496599</td>\n",
       "      <td>0.972991</td>\n",
       "      <td>0.963580</td>\n",
       "      <td>0.270370</td>\n",
       "      <td>0.350120</td>\n",
       "      <td>0.349453</td>\n",
       "      <td>0.920470</td>\n",
       "      <td>0.357842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NA</td>\n",
       "      <td>0.427136</td>\n",
       "      <td>0.974791</td>\n",
       "      <td>0.960483</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.360934</td>\n",
       "      <td>0.345491</td>\n",
       "      <td>0.802306</td>\n",
       "      <td>0.300137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MN</td>\n",
       "      <td>0.712082</td>\n",
       "      <td>0.995547</td>\n",
       "      <td>0.992119</td>\n",
       "      <td>0.661888</td>\n",
       "      <td>0.686068</td>\n",
       "      <td>0.682550</td>\n",
       "      <td>0.961264</td>\n",
       "      <td>0.721794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADP</td>\n",
       "      <td>0.880189</td>\n",
       "      <td>0.973365</td>\n",
       "      <td>0.971172</td>\n",
       "      <td>0.443356</td>\n",
       "      <td>0.589685</td>\n",
       "      <td>0.613110</td>\n",
       "      <td>0.982973</td>\n",
       "      <td>0.721988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZN</td>\n",
       "      <td>0.853968</td>\n",
       "      <td>0.996187</td>\n",
       "      <td>0.994112</td>\n",
       "      <td>0.768352</td>\n",
       "      <td>0.808901</td>\n",
       "      <td>0.807079</td>\n",
       "      <td>0.989792</td>\n",
       "      <td>0.879585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PO4</td>\n",
       "      <td>0.553942</td>\n",
       "      <td>0.987501</td>\n",
       "      <td>0.979681</td>\n",
       "      <td>0.448739</td>\n",
       "      <td>0.495822</td>\n",
       "      <td>0.488376</td>\n",
       "      <td>0.944524</td>\n",
       "      <td>0.469653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FE</td>\n",
       "      <td>0.817391</td>\n",
       "      <td>0.995613</td>\n",
       "      <td>0.993608</td>\n",
       "      <td>0.679518</td>\n",
       "      <td>0.742105</td>\n",
       "      <td>0.742131</td>\n",
       "      <td>0.991364</td>\n",
       "      <td>0.822914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>0.698784</td>\n",
       "      <td>0.989819</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.536088</td>\n",
       "      <td>0.606718</td>\n",
       "      <td>0.604683</td>\n",
       "      <td>0.961379</td>\n",
       "      <td>0.664219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CA</td>\n",
       "      <td>0.583655</td>\n",
       "      <td>0.994317</td>\n",
       "      <td>0.987880</td>\n",
       "      <td>0.620537</td>\n",
       "      <td>0.601531</td>\n",
       "      <td>0.595672</td>\n",
       "      <td>0.930920</td>\n",
       "      <td>0.604939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMP</td>\n",
       "      <td>0.688295</td>\n",
       "      <td>0.962877</td>\n",
       "      <td>0.954786</td>\n",
       "      <td>0.360186</td>\n",
       "      <td>0.472902</td>\n",
       "      <td>0.477708</td>\n",
       "      <td>0.942346</td>\n",
       "      <td>0.462122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NO2</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.985535</td>\n",
       "      <td>0.962645</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.140845</td>\n",
       "      <td>0.125581</td>\n",
       "      <td>0.745298</td>\n",
       "      <td>0.136813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GTP</td>\n",
       "      <td>0.748899</td>\n",
       "      <td>0.971088</td>\n",
       "      <td>0.964416</td>\n",
       "      <td>0.445026</td>\n",
       "      <td>0.558292</td>\n",
       "      <td>0.560977</td>\n",
       "      <td>0.962145</td>\n",
       "      <td>0.647657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GDP</td>\n",
       "      <td>0.816532</td>\n",
       "      <td>0.974398</td>\n",
       "      <td>0.969564</td>\n",
       "      <td>0.501859</td>\n",
       "      <td>0.621642</td>\n",
       "      <td>0.626306</td>\n",
       "      <td>0.976589</td>\n",
       "      <td>0.765284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LABind)",
   "language": "python",
   "name": "labind"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
