{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is an example of reproducing training results\n",
    "Prior to this step, please execute `download_weights.py` to download the weights for all pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # <-- add this at the top\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from readData import readData\n",
    "import shutil\n",
    "from utils import *\n",
    "import torch\n",
    "from torch import nn\n",
    "from model import LABind\n",
    "from func_help import setALlSeed,get_std_opt\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "from sklearn.model_selection import KFold\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "DEVICE = torch.device('cuda:0')\n",
    "root_path = getRootPath()\n",
    "dataset = 'LigBind' # DS1:LigBind, DS2:GPSite, DS3:Unseen\n",
    "\n",
    "nn_config = {\n",
    "    # dataset \n",
    "    'train_file': f'{root_path}/{dataset}/label/train/train.fa',\n",
    "    'test_file': f'{root_path}/{dataset}/label/test/test.fa',\n",
    "    'valid_file': f'{root_path}/{dataset}/label/picking.fa',\n",
    "    'proj_dir': f'{root_path}/{dataset}',\n",
    "    'lig_dict': pkl.load(open(f'{root_path}/tools/ligand.pkl', 'rb')),\n",
    "    'pdb_class':'source', # source or omegafold or esmfold\n",
    "    'dssp_max_repr': np.load(f'{root_path}/tools/dssp_max_repr.npy'),\n",
    "    'dssp_min_repr': np.load(f'{root_path}/tools/dssp_min_repr.npy'),\n",
    "    'ankh_max_repr': np.load(f'{root_path}/tools/ankh_max_repr.npy'),\n",
    "    'ankh_min_repr': np.load(f'{root_path}/tools/ankh_min_repr.npy'),\n",
    "    'esm2_max_repr': np.load(f'{root_path}/tools/esm2_max_repr.npy'),\n",
    "    'esm2_min_repr': np.load(f'{root_path}/tools/esm2_min_repr.npy'),\n",
    "    'ion_max_repr': np.load(f'{root_path}/tools/ion_max_repr.npy'),\n",
    "    'ion_min_repr': np.load(f'{root_path}/tools/ion_min_repr.npy'),\n",
    "    # model parameters\n",
    "    \n",
    "    'rfeat_dim':2580,\n",
    "    'ligand_dim':768, \n",
    "    'hidden_dim':256, \n",
    "    'heads':4, \n",
    "    'augment_eps':0.05, \n",
    "    'rbf_num':8, \n",
    "    'top_k':30, \n",
    "    'attn_drop':0.1, \n",
    "    'dropout':0.1, \n",
    "    'num_layers':4, \n",
    "    'lr':0.0004, \n",
    "    \n",
    "    # training parameters \n",
    "    # You can modify it according to the actual situation. \n",
    "    # Since it involves mapping the entire protein, it will consume a large amount of GPU memory.\n",
    "    'batch_size':1,\n",
    "    'max_patience':10,\n",
    "    'device_ids':[0,1], # 2*A100-40G\n",
    "}\n",
    "pretrain_path = { # Please modify \n",
    "    'esmfold_path': '../tools/esmfold_v1', # esmfold path\n",
    "    'esm2_path': '../tools/esm2', \n",
    "    'ankh_path': '../tools/ankh-large/', # ankh path\n",
    "    'molformer_path': '../tools/MoLFormer-XL-both-10pct/', # molformer path\n",
    "    'model_path':f'{root_path}/model/LigBind/' # based on Unseen\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train_file', 'test_file', 'valid_file', 'proj_dir', 'lig_dict', 'pdb_class', 'dssp_max_repr', 'dssp_min_repr', 'ankh_max_repr', 'ankh_min_repr', 'esm2_max_repr', 'esm2_min_repr', 'ion_max_repr', 'ion_min_repr', 'rfeat_dim', 'ligand_dim', 'hidden_dim', 'heads', 'augment_eps', 'rbf_num', 'top_k', 'attn_drop', 'dropout', 'num_layers', 'lr', 'batch_size', 'max_patience', 'device_ids'])\n"
     ]
    }
   ],
   "source": [
    "print(nn_config.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset\n",
    "Download the file from https://zenodo.org/records/13938443 and place it in the root directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### ESM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00,  2.04it/s]\n",
      "Some weights of EsmModel were not initialized from the model checkpoint at ../tools/esm2 and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda\n",
      "\n",
      "Processing CO3.fa (21 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 21/21 [00:00<00:00, 106120.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing SO4.fa (31 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 31/31 [00:00<00:00, 132948.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing MG.fa (665 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|███████████████████████████████▌      | 553/665 [00:00<00:00, 26351.10it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacty of 15.69 GiB of which 10.12 MiB is free. Including non-PyTorch memory, this process has 15.49 GiB memory in use. Of the allocated memory 15.14 GiB is allocated by PyTorch, and 79.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Run model on GPU\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 65\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mcpu()      \u001b[38;5;66;03m# shape: (1, L, D)\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m hidden[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;28mlen\u001b[39m(seq)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/transformers/models/esm/modeling_esm.py:914\u001b[0m, in \u001b[0;36mEsmModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    905\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    907\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    908\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    909\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    912\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    913\u001b[0m )\n\u001b[0;32m--> 914\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    926\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    927\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/transformers/models/esm/modeling_esm.py:619\u001b[0m, in \u001b[0;36mEsmEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    608\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    609\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    610\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    616\u001b[0m         output_attentions,\n\u001b[1;32m    617\u001b[0m     )\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/transformers/models/esm/modeling_esm.py:509\u001b[0m, in \u001b[0;36mEsmLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    499\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    506\u001b[0m ):\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    508\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/transformers/models/esm/modeling_esm.py:443\u001b[0m, in \u001b[0;36mEsmAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    434\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    441\u001b[0m ):\n\u001b[1;32m    442\u001b[0m     hidden_states_ln \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states)\n\u001b[0;32m--> 443\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states_ln\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    453\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/transformers/models/esm/modeling_esm.py:344\u001b[0m, in \u001b[0;36mEsmSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    341\u001b[0m     past_key_value \u001b[38;5;241m=\u001b[39m (key_layer, value_layer)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrotary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 344\u001b[0m     query_layer, key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotary_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[1;32m    347\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(query_layer, key_layer\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/transformers/models/esm/modeling_esm.py:125\u001b[0m, in \u001b[0;36mRotaryEmbedding.forward\u001b[0;34m(self, q, k)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, q: torch\u001b[38;5;241m.\u001b[39mTensor, k: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cos_cached, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sin_cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_cos_sin_tables(k, seq_dimension\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 125\u001b[0m         \u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cos_cached\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sin_cached\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    126\u001b[0m         apply_rotary_pos_emb(k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cos_cached, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sin_cached),\n\u001b[1;32m    127\u001b[0m     )\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/transformers/models/esm/modeling_esm.py:60\u001b[0m, in \u001b[0;36mapply_rotary_pos_emb\u001b[0;34m(x, cos, sin)\u001b[0m\n\u001b[1;32m     57\u001b[0m cos \u001b[38;5;241m=\u001b[39m cos[:, :, : x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], :]\n\u001b[1;32m     58\u001b[0m sin \u001b[38;5;241m=\u001b[39m sin[:, :, : x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], :]\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m) \u001b[38;5;241m+\u001b[39m (rotate_half(x) \u001b[38;5;241m*\u001b[39m sin)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacty of 15.69 GiB of which 10.12 MiB is free. Including non-PyTorch memory, this process has 15.49 GiB memory in use. Of the allocated memory 15.14 GiB is allocated by PyTorch, and 79.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from Bio import SeqIO\n",
    "from transformers import AutoTokenizer, AutoModel \n",
    "from Bio import SeqIO\n",
    "# 1️⃣ Delete all tensors and models\n",
    "del model\n",
    "gc.collect()\n",
    "\n",
    "# 2️⃣ Empty PyTorch cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 3️⃣ Optional: reset CUDA memory stats\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "MODEL_DIR = \"../tools/esm2\"   # <-- directory containing HF files\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 4\n",
    "FASTA_ROOT = \"fasta\"             # root directory containing subdirs of FASTA files\n",
    "SAVE_ROOT = \"embeddings\"         # root output folder\n",
    "out_path = f\"{root_path}/{dataset}/esm/\"\n",
    "fasta_path = f\"{root_path}/{dataset}/fasta/\"\n",
    "\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrain_path['esm2_path'])\n",
    "model     = AutoModel.from_pretrained(pretrain_path['esm2_path'])\n",
    "model.to(DEVICE)\n",
    "\n",
    "model.eval()\n",
    "model.gradient_checkpointing_enable()\n",
    "print(\"Model loaded on\", DEVICE)\n",
    "for file_class in os.listdir(fasta_path):\n",
    "    class_path = os.path.join(fasta_path, file_class)\n",
    "\n",
    "    for fasta_file in os.listdir(class_path):\n",
    "        fasta_path_full = os.path.join(class_path, fasta_file)\n",
    "\n",
    "        sequences = list(SeqIO.parse(fasta_path_full, \"fasta\"))\n",
    "\n",
    "        print(f\"\\nProcessing {fasta_file} ({len(sequences)} sequences)\")\n",
    "\n",
    "        for record in tqdm(sequences):\n",
    "            save_path = os.path.join(out_path, f\"{record.id}.npy\")\n",
    "\n",
    "            # Skip if already processed\n",
    "            if os.path.exists(save_path):\n",
    "                continue\n",
    "\n",
    "            seq = str(record.seq)\n",
    "\n",
    "            # Tokenize for HF model\n",
    "            encoded = tokenizer(\n",
    "                seq,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=False,\n",
    "                add_special_tokens=True\n",
    "            )\n",
    "\n",
    "            encoded = {k: v.to(DEVICE) for k, v in encoded.items()}\n",
    "\n",
    "            # Run model on GPU\n",
    "            with torch.no_grad():\n",
    "                output = model(**encoded)\n",
    "                hidden = output.last_hidden_state.cpu()      # shape: (1, L, D)\n",
    "                embedding = hidden[0, 1:len(seq)+1].numpy()\n",
    "\n",
    "            # Save\n",
    "            np.save(save_path, embedding)\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        \n",
    "\n",
    "# Cleanup\n",
    "del model\n",
    "gc.collect()\n",
    "print(\"\\n✓ Done extracting ESM embeddings!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping bad file: 6s9fB.npy Error: Failed to interpret file '/virtual/zengzix4/LABind_ESM/LigBind/esm3B/6s9fB.npy' as a pickle\n",
      "Skipping bad file: 6ptkA.npy Error: cannot reshape array of size 1138656 into shape (471,2560)\n",
      "FINAL ESM MIN SHAPE: (2560,)\n",
      "FINAL ESM MAX SHAPE: (2560,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "esm_dir = \"/virtual/zengzix4/LABind_ESM/LigBind/esm3B\"    # <-- CHANGE THIS\n",
    "save_file = \"esm2_repr_stats.npy\"\n",
    "save_dir = \"/virtual/zengzix4/LABind_ESM/tools/\"\n",
    "# Use None so we can initialize on first file\n",
    "\n",
    "all_min = None\n",
    "all_max = None\n",
    "\n",
    "for f in os.listdir(esm_dir):\n",
    "    if not f.endswith(\".npy\"):\n",
    "        continue\n",
    "\n",
    "    fpath = os.path.join(esm_dir, f)\n",
    "\n",
    "    try:\n",
    "        arr = np.load(fpath, allow_pickle=True)\n",
    "    except Exception as e:\n",
    "        print(\"Skipping bad file:\", f, \"Error:\", e)\n",
    "        continue\n",
    "\n",
    "    # Convert object arrays to real numpy if possible\n",
    "    if isinstance(arr, np.ndarray) and arr.dtype == object:\n",
    "        try:\n",
    "            arr = np.vstack(arr)  # many ESM embeddings come as list of arrays\n",
    "        except:\n",
    "            print(\"Skipping malformed object array:\", f)\n",
    "            continue\n",
    "\n",
    "    # Now check shape\n",
    "    if arr.ndim != 2:\n",
    "        print(\"Skipping wrong shape\", f, arr.shape)\n",
    "        continue\n",
    "\n",
    "    # Compute per-dimension min/max\n",
    "    fmin = arr.min(axis=0)\n",
    "    fmax = arr.max(axis=0)\n",
    "\n",
    "    if all_min is None:\n",
    "        all_min = fmin\n",
    "        all_max = fmax\n",
    "    else:\n",
    "        all_min = np.minimum(all_min, fmin)\n",
    "        all_max = np.maximum(all_max, fmax)\n",
    "\n",
    "print(\"FINAL ESM MIN SHAPE:\", all_min.shape)\n",
    "print(\"FINAL ESM MAX SHAPE:\", all_max.shape)\n",
    "np.save(os.path.join(save_dir, \"esm2_min_repr.npy\"), all_min)\n",
    "np.save(os.path.join(save_dir, \"esm2_max_repr.npy\"), all_max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Ankh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/virtual/zengzix4/miniconda/envs/LABind/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ankh features\n",
    "from transformers import AutoTokenizer, T5EncoderModel \n",
    "from Bio import SeqIO\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrain_path['ankh_path'])\n",
    "model     = T5EncoderModel.from_pretrained(pretrain_path['ankh_path'])\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "out_path = f\"{root_path}/{dataset}/ankh/\"\n",
    "makeDir(out_path)\n",
    "# 使用biopython读取fasta文件\n",
    "fasta_path = f\"{root_path}/{dataset}/fasta/\"\n",
    "for file_class in os.listdir(fasta_path):\n",
    "    for fasta_base_file in os.listdir(f\"{fasta_path}/{file_class}\"):\n",
    "        fasta_file = f\"{fasta_path}/{file_class}/{fasta_base_file}\"\n",
    "        sequences = SeqIO.parse(fasta_file, \"fasta\")\n",
    "        for record in tqdm(sequences):\n",
    "            if os.path.exists(out_path+f'{record.id}.npy'):\n",
    "                continue\n",
    "            ids = tokenizer.batch_encode_plus([list(record.seq)], add_special_tokens=True, padding=True, is_split_into_words=True, return_tensors=\"pt\")\n",
    "            input_ids = ids['input_ids'].to(DEVICE)\n",
    "            attention_mask = ids['attention_mask'].to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                embedding_repr = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "                emb = embedding_repr.last_hidden_state[0,:len(record.seq)].cpu().numpy()\n",
    "                np.save(out_path+f'{record.id}.npy',emb)\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### DSSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DSSP running: 100%|███████████████| 11121/11121 [1:22:02<00:00,  2.26proteins/s]\n"
     ]
    }
   ],
   "source": [
    "from Bio.PDB import PDBParser\n",
    "from Bio.PDB.DSSP import DSSP\n",
    "\n",
    "mapSS = {' ':[0,0,0,0,0,0,0,0,0],\n",
    "        '-':[1,0,0,0,0,0,0,0,0],\n",
    "        'H':[0,1,0,0,0,0,0,0,0],\n",
    "        'B':[0,0,1,0,0,0,0,0,0],\n",
    "        'E':[0,0,0,1,0,0,0,0,0],\n",
    "        'G':[0,0,0,0,1,0,0,0,0],\n",
    "        'I':[0,0,0,0,0,1,0,0,0],\n",
    "        'P':[0,0,0,0,0,0,1,0,0],\n",
    "        'T':[0,0,0,0,0,0,0,1,0],\n",
    "        'S':[0,0,0,0,0,0,0,0,1]}\n",
    "p = PDBParser(QUIET=True)\n",
    "pdb_path = f\"{root_path}/{dataset}/pdb/\"\n",
    "dssp_path = \"../tools/mkdssp\"\n",
    "pdb_class = nn_config['pdb_class']\n",
    "makeDir(f\"{root_path}/{dataset}/{pdb_class}_dssp/\")\n",
    "test_files = os.listdir(pdb_path)\n",
    "pdb_files = []\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(pdb_path):\n",
    "    for f in filenames:\n",
    "        if f.endswith('.pdb'):\n",
    "            pdb_files.append(os.path.join(dirpath, f)) \n",
    "            \n",
    "for pdb_file_name in tqdm(pdb_files, desc='DSSP running',ncols=80,unit='proteins'):\n",
    "    pdb_file = pdb_file_name\n",
    "    save_file = pdb_file.replace('.pdb','.npy').replace('pdb',f'{pdb_class}_dssp')\n",
    "    if os.path.exists(save_file):\n",
    "        continue\n",
    "    structure = p.get_structure(\"tmp\", pdb_file)\n",
    "    model = structure[0]\n",
    "    try:\n",
    "        dssp = DSSP(model, pdb_file, dssp=dssp_path)\n",
    "        keys = list(dssp.keys())\n",
    "    except:\n",
    "        keys = []\n",
    "    res_np = []\n",
    "    for chain in model:\n",
    "        for residue in chain:\n",
    "            res_key = (chain.id,(' ', residue.id[1], residue.id[2]))\n",
    "            if res_key in keys:\n",
    "                tuple_dssp = dssp[res_key]\n",
    "                res_np.append(mapSS[tuple_dssp[2]] + list(tuple_dssp[3:]))\n",
    "            else:\n",
    "                res_np.append(np.zeros(20))\n",
    "    os.makedirs(os.path.dirname(save_file), exist_ok=True)\n",
    "    np.save(save_file, np.array(res_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB.ResidueDepth import get_surface\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "pdb_path = f\"{root_path}/{dataset}/pdb/\"\n",
    "msms_path = \"../tools/msms\"\n",
    "pdb_class = nn_config['pdb_class']\n",
    "makeDir(f\"{root_path}/{dataset}/{pdb_class}_pos/\")\n",
    "pdb_files = []\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(pdb_path):\n",
    "    for f in filenames:\n",
    "        if f.endswith('.pdb'):\n",
    "            pdb_files.append(os.path.join(dirpath, f)) \n",
    "\n",
    "for pdb_file in tqdm(pdb_files,desc='MSMS running',ncols=80,unit='proteins'):\n",
    "    save_file = pdb_file.replace('.pdb','.npy').replace('pdb',f'{pdb_class}_pos')\n",
    "    if os.path.exists(save_file):\n",
    "        continue\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    X = []\n",
    "    chain_atom = ['N', 'CA', 'C', 'O']\n",
    "    model = parser.get_structure('model', pdb_file)[0]\n",
    "    chain = next(model.get_chains())\n",
    "    try:\n",
    "        surf = get_surface(chain,MSMS=msms_path)\n",
    "        surf_tree = cKDTree(surf)\n",
    "    except:\n",
    "        surf = np.empty(0)\n",
    "    for residue in chain:\n",
    "        line = []\n",
    "        atoms_coord = np.array([atom.get_coord() for atom in residue])\n",
    "        if surf.size == 0:\n",
    "            dist, _ = surf_tree.query(atoms_coord)\n",
    "            closest_atom = np.argmin(dist)\n",
    "            closest_pos = atoms_coord[closest_atom]\n",
    "        else:\n",
    "            closest_pos = atoms_coord[-1]\n",
    "        atoms = list(residue.get_atoms())\n",
    "        try:\n",
    "            ca_pos = residue['CA'].get_coord()\n",
    "        except KeyError:\n",
    "            print(residue)\n",
    "            continue\n",
    "\n",
    "        pos_s = 0\n",
    "        un_s = 0\n",
    "        for atom in atoms:\n",
    "            if atom.name in chain_atom:\n",
    "                line.append(atom.get_coord())\n",
    "            else:\n",
    "                pos_s += calMass(atom,True)\n",
    "                un_s += calMass(atom,False)\n",
    "        # 此处line应该等于4\n",
    "        if len(line) != 4:\n",
    "            line = line + [list(ca_pos)]*(4-len(line))\n",
    "        if un_s == 0:\n",
    "            R_pos = ca_pos\n",
    "        else:\n",
    "            R_pos = pos_s / un_s\n",
    "        line.append(R_pos)  \n",
    "        line.append(closest_pos) # 加入最近点的残基信息\n",
    "        X.append(line) \n",
    "    \n",
    "    os.makedirs(os.path.dirname(save_file), exist_ok=True)\n",
    "\n",
    "    np.save(save_file, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, valid_list, fold_idx):\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    valid_data = readData(\n",
    "        name_list=valid_list, \n",
    "        proj_dir=nn_config['proj_dir'], \n",
    "        lig_dict=nn_config['lig_dict'],\n",
    "        true_file=nn_config['train_file'], mode='train') # If 5-fold cross-validation is not used, it needs to be changed to valid_file.\n",
    "    valid_loader = DataLoader(valid_data, batch_size=nn_config['batch_size'],shuffle=True, collate_fn=valid_data.collate_fn, num_workers=5)\n",
    "    all_y_score = []\n",
    "    all_y_true = []\n",
    "    with torch.no_grad():\n",
    "        for rfeat, ligand, xyz,  mask, y_true in valid_loader:\n",
    "            tensors = [rfeat, ligand, xyz,  mask, y_true]\n",
    "            tensors = [tensor.to(DEVICE) for tensor in tensors]\n",
    "            rfeat, ligand, xyz, mask, y_true = tensors\n",
    "            logits = model(rfeat, ligand, xyz,  mask).sigmoid() # [N]\n",
    "            logits = torch.masked_select(logits, mask==1)\n",
    "            y_true = torch.masked_select(y_true, mask==1)\n",
    "            all_y_score.extend(logits.cpu().detach().numpy())\n",
    "            all_y_true.extend(y_true.cpu().detach().numpy())\n",
    "        # 通过aupr数值进行早停\n",
    "        aupr_value = average_precision_score(all_y_true, all_y_score)\n",
    "    return aupr_value\n",
    "\n",
    "def train(train_list,valid_list=None,model=None,epochs=50,fold_idx=None):\n",
    "    model.to(DEVICE)\n",
    "    train_data = readData(\n",
    "        name_list=train_list, \n",
    "        proj_dir=nn_config['proj_dir'], \n",
    "        lig_dict=nn_config['lig_dict'],\n",
    "        true_file=nn_config['train_file'], mode='train')\n",
    "    train_loader = DataLoader(train_data, batch_size=nn_config['batch_size'],shuffle=True, collate_fn=train_data.collate_fn, num_workers=5)\n",
    "    loss_fn = nn.BCELoss(reduction='none')\n",
    "    optimizer = get_std_opt(len(train_list),nn_config['batch_size'], model.parameters(), nn_config['hidden_dim'], nn_config['lr'])\n",
    "    v_max_aupr = 0\n",
    "    patience = 0\n",
    "    t_mccs = []\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model,device_ids=nn_config['device_ids'])\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        all_loss = 0\n",
    "        all_cnt = 0\n",
    "        model.train()\n",
    "        for rfeat, ligand, xyz,  mask, y_true in tqdm(train_loader):\n",
    "            tensors = [rfeat, ligand, xyz,  mask, y_true]\n",
    "            tensors = [tensor.to(DEVICE) for tensor in tensors]\n",
    "            rfeat, ligand, xyz, mask, y_true = tensors\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(rfeat, ligand, xyz, mask).sigmoid() # [N]\n",
    "            # 计算所有离子的loss\n",
    "            loss = loss_fn(logits, y_true) * mask\n",
    "            loss = loss.sum() / mask.sum()\n",
    "            all_loss += loss.item()\n",
    "            all_cnt += 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_losses.append(all_loss / all_cnt)\n",
    "        # 根据验证集的aupr进行早停\n",
    "        if valid_list is not None:\n",
    "            v_aupr = valid(model,valid_list, fold_idx)\n",
    "            t_mccs.append(v_aupr)\n",
    "            print(f'Epoch {epoch} Loss: {all_loss / all_cnt}', f'Epoch Valid {epoch} AUPR: {v_aupr}')\n",
    "            if v_aupr > v_max_aupr:\n",
    "                v_max_aupr = v_aupr\n",
    "                patience = 0\n",
    "                torch.save(model.state_dict(), f'{root_path}/Output/{dataset}_5fold/fold{fold_idx}.ckpt')\n",
    "            else:\n",
    "                patience += 1\n",
    "            if patience >= nn_config['max_patience']:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 6624/6624 [05:35<00:00, 19.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 0.11012470396291825 Epoch Valid 0 AUPR: 0.3649063092848656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 6624/6624 [05:35<00:00, 19.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.0808964598567716 Epoch Valid 1 AUPR: 0.4042977795973669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 6624/6624 [05:35<00:00, 19.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.07740467152736687 Epoch Valid 2 AUPR: 0.39869388071906287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 6624/6624 [05:36<00:00, 19.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.07640924130606032 Epoch Valid 3 AUPR: 0.45190840273588584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 6624/6624 [05:36<00:00, 19.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.07633571044025654 Epoch Valid 4 AUPR: 0.4765048032054525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 6624/6624 [05:35<00:00, 19.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 0.07693343534816265 Epoch Valid 5 AUPR: 0.46147074653571196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 6624/6624 [05:35<00:00, 19.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 0.07672491146806992 Epoch Valid 6 AUPR: 0.468495073245951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 6624/6624 [05:35<00:00, 19.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: 0.0781396843227711 Epoch Valid 7 AUPR: 0.43223980948417773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 6624/6624 [05:35<00:00, 19.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss: 0.07807417598905446 Epoch Valid 8 AUPR: 0.434281666641634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 6624/6624 [05:36<00:00, 19.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss: 0.079348838853504 Epoch Valid 9 AUPR: 0.4102077371613678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 6624/6624 [05:35<00:00, 19.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss: 0.08062201023735482 Epoch Valid 10 AUPR: 0.4311877065979482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 6624/6624 [05:35<00:00, 19.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Loss: 0.07980137165305208 Epoch Valid 11 AUPR: 0.30133113641328224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 6624/6624 [05:36<00:00, 19.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Loss: 0.07818891937433697 Epoch Valid 12 AUPR: 0.46849111079468087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 6624/6624 [05:35<00:00, 19.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Loss: 0.07676619822113012 Epoch Valid 13 AUPR: 0.38623831800792496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 6624/6624 [05:35<00:00, 19.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Loss: 0.07556277545550755 Epoch Valid 14 AUPR: 0.42395304228508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|████████████▍                          | 2123/6624 [01:47<03:47, 19.76it/s]"
     ]
    }
   ],
   "source": [
    "setALlSeed(11)\n",
    "# 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state = 42)\n",
    "data_list = readDataList(f'{root_path}/{dataset}/label/train/train.fa',skew=1)\n",
    "makeDir(f'{root_path}/Output/{dataset}_5fold/')\n",
    "fold_idx = 0\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "for train_idx, valid_idx in kf.split(data_list):\n",
    "    train_list = [data_list[i] for i in train_idx]\n",
    "    valid_list = [data_list[j] for j in valid_idx]\n",
    "    model = LABind(\n",
    "    rfeat_dim=nn_config['rfeat_dim'], ligand_dim=nn_config['ligand_dim'], hidden_dim=nn_config['hidden_dim'], heads=nn_config['heads'], augment_eps=nn_config['augment_eps'], \n",
    "    rbf_num=nn_config['rbf_num'],top_k=nn_config['top_k'], attn_drop=nn_config['attn_drop'], dropout=nn_config['dropout'], num_layers=nn_config['num_layers'])\n",
    "    train(train_list,valid_list,model,epochs=70,fold_idx=fold_idx)\n",
    "    fold_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setALlSeed(11)\n",
    "# train-validation-test\n",
    "train_list = readDataList(f'{root_path}/{dataset}/label/train/train.fa',skew=1)\n",
    "valid_list = readDataList(f'{root_path}/{dataset}/label/picking.fa',skew=1)\n",
    "makeDir(f'{root_path}/Output/{dataset}/')\n",
    "model = LABind(\n",
    "rfeat_dim=nn_config['rfeat_dim'], ligand_dim=nn_config['ligand_dim'], hidden_dim=nn_config['hidden_dim'], heads=nn_config['heads'], augment_eps=nn_config['augment_eps'], \n",
    "rbf_num=nn_config['rbf_num'],top_k=nn_config['top_k'], attn_drop=nn_config['attn_drop'], dropout=nn_config['dropout'], num_layers=nn_config['num_layers'])\n",
    "train(train_list, valid_list, model, epochs=70, fold_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the best threshold for MCC based on the validation set.\n",
    "model_path = f'{root_path}/Output/{dataset}_5fold/' # if 5-fold cross-validation, {dataset}_5fold\n",
    "print(model_path)\n",
    "print(nn_config['pdb_class'])\n",
    "\n",
    "models = []\n",
    "for fold in range(5): # if 5-fold cross-validation, set to 5\n",
    "    state_dict = torch.load(model_path + 'fold%s.ckpt'%fold,'cuda:0')\n",
    "    model = LABind(\n",
    "        rfeat_dim=nn_config['rfeat_dim'], ligand_dim=nn_config['ligand_dim'], hidden_dim=nn_config['hidden_dim'], heads=nn_config['heads'], augment_eps=nn_config['augment_eps'], \n",
    "        rbf_num=nn_config['rbf_num'],top_k=nn_config['top_k'], attn_drop=nn_config['attn_drop'], dropout=nn_config['dropout'], num_layers=nn_config['num_layers']).to(device)\n",
    "    model = nn.DataParallel(model,device_ids=nn_config['device_ids'])\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "    \n",
    "valid_list = readDataList(f'{root_path}/{dataset}/label/picking.fa',skew=1)\n",
    "valid_data = readData(\n",
    "    name_list=valid_list, \n",
    "    proj_dir=nn_config['proj_dir'], \n",
    "    lig_dict=nn_config['lig_dict'],\n",
    "    true_file=f'{root_path}/{dataset}/label/picking.fa', mode='train')\n",
    "# 打印长度\n",
    "valid_loader = DataLoader(valid_data, batch_size=nn_config['batch_size'], collate_fn=valid_data.collate_fn)\n",
    "print(f'valid data length: {len(valid_data)}')\n",
    "all_y_score = []\n",
    "all_y_true = []\n",
    "with torch.no_grad():\n",
    "    for rfeat, ligand, xyz,  mask, y_true in valid_loader:\n",
    "        tensors = [rfeat, ligand, xyz,  mask, y_true]\n",
    "        tensors = [tensor.to(DEVICE) for tensor in tensors]\n",
    "        rfeat, ligand, xyz, mask, y_true = tensors\n",
    "        \n",
    "        logits = [model(rfeat, ligand, xyz, mask).sigmoid() for model in models]\n",
    "        logits = torch.stack(logits,0).mean(0)\n",
    "        \n",
    "        logits = torch.masked_select(logits, mask==1)\n",
    "        y_true = torch.masked_select(y_true, mask==1)\n",
    "        all_y_score.extend(logits.cpu().detach().numpy())\n",
    "        all_y_true.extend(y_true.cpu().detach().numpy())\n",
    "\n",
    "best_threshold,best_mcc,best_pred = getBestThreshold(all_y_true, all_y_score)\n",
    "appendText(f'{model_path}/Best_Threshold.txt',f'{best_threshold} {best_mcc}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "model_path = f'{root_path}/Output/{dataset}/' # if 5-fold cross-validation, {dataset}_5fold\n",
    "print(model_path)\n",
    "print(nn_config['pdb_class'])\n",
    "\n",
    "models = []\n",
    "for fold in range(1): # if 5-fold cross-validation, set to 5\n",
    "    state_dict = torch.load(model_path + 'fold%s.ckpt'%fold,'cuda:0')\n",
    "    model = LABind(\n",
    "        rfeat_dim=nn_config['rfeat_dim'], ligand_dim=nn_config['ligand_dim'], hidden_dim=nn_config['hidden_dim'], heads=nn_config['heads'], augment_eps=nn_config['augment_eps'], \n",
    "        rbf_num=nn_config['rbf_num'],top_k=nn_config['top_k'], attn_drop=nn_config['attn_drop'], dropout=nn_config['dropout'], num_layers=nn_config['num_layers']).to(device)\n",
    "    model = nn.DataParallel(model,device_ids=nn_config['device_ids'])\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "\n",
    "df = pd.DataFrame(columns=['ligand','Rec','SPE','Acc','Pre','F1','MCC','AUC','AUPR'])\n",
    "for ionic in os.listdir(f'{root_path}/{dataset}/label/test/'):\n",
    "    ionic = ionic.split('.')[0]\n",
    "    test_list = readDataList(f'{root_path}/{dataset}/label/test/{ionic}.fa',skew=1)\n",
    "    test_data = readData(\n",
    "        name_list=test_list, \n",
    "        proj_dir=nn_config['proj_dir'], \n",
    "        lig_dict=nn_config['lig_dict'],\n",
    "        true_file=f'{root_path}/{dataset}/label/test/{ionic}.fa', mode='test')\n",
    "    # 打印长度\n",
    "    test_loader = DataLoader(test_data, batch_size=nn_config['batch_size'], collate_fn=test_data.collate_fn)\n",
    "    print(f'{ionic} test data length: {len(test_data)}')\n",
    "    all_y_score = []\n",
    "    all_y_true = []\n",
    "    with torch.no_grad():\n",
    "        for rfeat, ligand, xyz,  mask, y_true in test_loader:\n",
    "            tensors = [rfeat, ligand, xyz,  mask, y_true]\n",
    "            tensors = [tensor.to(DEVICE) for tensor in tensors]\n",
    "            rfeat, ligand, xyz, mask, y_true = tensors\n",
    "            \n",
    "            logits = [model(rfeat, ligand, xyz, mask).sigmoid() for model in models]\n",
    "            logits = torch.stack(logits,0).mean(0)\n",
    "            \n",
    "            logits = torch.masked_select(logits, mask==1)\n",
    "            y_true = torch.masked_select(y_true, mask==1)\n",
    "            all_y_score.extend(logits.cpu().detach().numpy())\n",
    "            all_y_true.extend(y_true.cpu().detach().numpy())\n",
    "    data_dict = calEval(all_y_true, all_y_score, best_th = best_threshold) # please set best threshold.\n",
    "    data_dict['ligand'] = ionic\n",
    "    df = pd.concat([df,pd.DataFrame(data_dict,index=[0])])\n",
    "df.to_csv(f'{model_path}test.csv',index=False)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LABind)",
   "language": "python",
   "name": "labind"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
