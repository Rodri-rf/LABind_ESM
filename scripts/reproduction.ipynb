{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is an example of reproducing training results\n",
    "Prior to this step, please execute `download_weights.py` to download the weights for all pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # <-- add this at the top\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import readData\n",
    "import shutil\n",
    "from utils import *\n",
    "import torch\n",
    "from torch import nn\n",
    "from model import LABind\n",
    "from func_help import setALlSeed,get_std_opt\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "from sklearn.model_selection import KFold\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "DEVICE = torch.device('cuda:0')\n",
    "root_path = getRootPath()\n",
    "dataset = 'LigBind' # DS1:LigBind, DS2:GPSite, DS3:Unseen\n",
    "\n",
    "nn_config = {\n",
    "    # dataset \n",
    "    'train_file': f'{root_path}/{dataset}/label/train/train.fa',\n",
    "    'test_file': f'{root_path}/{dataset}/label/test/test.fa',\n",
    "    'valid_file': f'{root_path}/{dataset}/label/picking.fa',\n",
    "    'proj_dir': f'{root_path}/{dataset}',\n",
    "    'lig_dict': pkl.load(open(f'{root_path}/tools/ligand.pkl', 'rb')),\n",
    "    'pdb_class':'source', # source or omegafold or esmfold\n",
    "    'dssp_max_repr': np.load(f'{root_path}/tools/dssp_max_repr.npy'),\n",
    "    'dssp_min_repr': np.load(f'{root_path}/tools/dssp_min_repr.npy'),\n",
    "    'ankh_max_repr': np.load(f'{root_path}/tools/ankh_max_repr.npy'),\n",
    "    'ankh_min_repr': np.load(f'{root_path}/tools/ankh_min_repr.npy'),\n",
    "    'esm2_max_repr': np.load(f'{root_path}/tools/esm2_max_repr.npy'),\n",
    "    'esm2_min_repr': np.load(f'{root_path}/tools/esm2_min_repr.npy'),\n",
    "    'ion_max_repr': np.load(f'{root_path}/tools/ion_max_repr.npy'),\n",
    "    'ion_min_repr': np.load(f'{root_path}/tools/ion_min_repr.npy'),\n",
    "    # model parameters\n",
    "    \n",
    "    'rfeat_dim':2580,\n",
    "    'ligand_dim':768, \n",
    "    'hidden_dim':256, \n",
    "    'heads':4, \n",
    "    'augment_eps':0.05, \n",
    "    'rbf_num':8, \n",
    "    'top_k':30, \n",
    "    'attn_drop':0.1, \n",
    "    'dropout':0.1, \n",
    "    'num_layers':4, \n",
    "    'lr':0.0004, \n",
    "    \n",
    "    # training parameters \n",
    "    # You can modify it according to the actual situation. \n",
    "    # Since it involves mapping the entire protein, it will consume a large amount of GPU memory.\n",
    "    'batch_size':1,\n",
    "    'max_patience':10,\n",
    "    'device_ids':[0]\n",
    "}\n",
    "pretrain_path = { # Please modify \n",
    "    'esmfold_path': '../tools/esmfold_v1', # esmfold path\n",
    "    'esm2_path': '../tools/esm2', \n",
    "    'ankh_path': '../tools/ankh-large/', # ankh path\n",
    "    'molformer_path': '../tools/MoLFormer-XL-both-10pct/', # molformer path\n",
    "    'model_path':f'{root_path}/model/LigBind/' # based on Unseen\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train_file', 'test_file', 'valid_file', 'proj_dir', 'lig_dict', 'pdb_class', 'dssp_max_repr', 'dssp_min_repr', 'ankh_max_repr', 'ankh_min_repr', 'esm2_max_repr', 'esm2_min_repr', 'ion_max_repr', 'ion_min_repr', 'rfeat_dim', 'ligand_dim', 'hidden_dim', 'heads', 'augment_eps', 'rbf_num', 'top_k', 'attn_drop', 'dropout', 'num_layers', 'lr', 'batch_size', 'max_patience', 'device_ids'])\n"
     ]
    }
   ],
   "source": [
    "print(nn_config.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset\n",
    "Download the file from https://zenodo.org/records/13938443 and place it in the root directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### ESM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00,  2.04it/s]\n",
      "Some weights of EsmModel were not initialized from the model checkpoint at ../tools/esm2 and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda\n",
      "\n",
      "Processing CO3.fa (21 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 21/21 [00:00<00:00, 106120.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing SO4.fa (31 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 31/31 [00:00<00:00, 132948.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing MG.fa (665 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|███████████████████████████████▌      | 553/665 [00:00<00:00, 26351.10it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacty of 15.69 GiB of which 10.12 MiB is free. Including non-PyTorch memory, this process has 15.49 GiB memory in use. Of the allocated memory 15.14 GiB is allocated by PyTorch, and 79.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Run model on GPU\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 65\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mcpu()      \u001b[38;5;66;03m# shape: (1, L, D)\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m hidden[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;28mlen\u001b[39m(seq)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/transformers/models/esm/modeling_esm.py:914\u001b[0m, in \u001b[0;36mEsmModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    905\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    907\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    908\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    909\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    912\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    913\u001b[0m )\n\u001b[0;32m--> 914\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    926\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    927\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/transformers/models/esm/modeling_esm.py:619\u001b[0m, in \u001b[0;36mEsmEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    608\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    609\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    610\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    616\u001b[0m         output_attentions,\n\u001b[1;32m    617\u001b[0m     )\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/transformers/models/esm/modeling_esm.py:509\u001b[0m, in \u001b[0;36mEsmLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    499\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    506\u001b[0m ):\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    508\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/transformers/models/esm/modeling_esm.py:443\u001b[0m, in \u001b[0;36mEsmAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    434\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    441\u001b[0m ):\n\u001b[1;32m    442\u001b[0m     hidden_states_ln \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states)\n\u001b[0;32m--> 443\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states_ln\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    453\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/transformers/models/esm/modeling_esm.py:344\u001b[0m, in \u001b[0;36mEsmSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    341\u001b[0m     past_key_value \u001b[38;5;241m=\u001b[39m (key_layer, value_layer)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrotary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 344\u001b[0m     query_layer, key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotary_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[1;32m    347\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(query_layer, key_layer\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/transformers/models/esm/modeling_esm.py:125\u001b[0m, in \u001b[0;36mRotaryEmbedding.forward\u001b[0;34m(self, q, k)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, q: torch\u001b[38;5;241m.\u001b[39mTensor, k: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cos_cached, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sin_cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_cos_sin_tables(k, seq_dimension\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 125\u001b[0m         \u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cos_cached\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sin_cached\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    126\u001b[0m         apply_rotary_pos_emb(k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cos_cached, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sin_cached),\n\u001b[1;32m    127\u001b[0m     )\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/transformers/models/esm/modeling_esm.py:60\u001b[0m, in \u001b[0;36mapply_rotary_pos_emb\u001b[0;34m(x, cos, sin)\u001b[0m\n\u001b[1;32m     57\u001b[0m cos \u001b[38;5;241m=\u001b[39m cos[:, :, : x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], :]\n\u001b[1;32m     58\u001b[0m sin \u001b[38;5;241m=\u001b[39m sin[:, :, : x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], :]\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m) \u001b[38;5;241m+\u001b[39m (rotate_half(x) \u001b[38;5;241m*\u001b[39m sin)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacty of 15.69 GiB of which 10.12 MiB is free. Including non-PyTorch memory, this process has 15.49 GiB memory in use. Of the allocated memory 15.14 GiB is allocated by PyTorch, and 79.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from Bio import SeqIO\n",
    "from transformers import AutoTokenizer, AutoModel \n",
    "from Bio import SeqIO\n",
    "# 1️⃣ Delete all tensors and models\n",
    "del model\n",
    "gc.collect()\n",
    "\n",
    "# 2️⃣ Empty PyTorch cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 3️⃣ Optional: reset CUDA memory stats\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "MODEL_DIR = \"../tools/esm2\"   # <-- directory containing HF files\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 4\n",
    "FASTA_ROOT = \"fasta\"             # root directory containing subdirs of FASTA files\n",
    "SAVE_ROOT = \"embeddings\"         # root output folder\n",
    "out_path = f\"{root_path}/{dataset}/esm/\"\n",
    "fasta_path = f\"{root_path}/{dataset}/fasta/\"\n",
    "\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrain_path['esm2_path'])\n",
    "model     = AutoModel.from_pretrained(pretrain_path['esm2_path'])\n",
    "model.to(DEVICE)\n",
    "\n",
    "model.eval()\n",
    "model.gradient_checkpointing_enable()\n",
    "print(\"Model loaded on\", DEVICE)\n",
    "for file_class in os.listdir(fasta_path):\n",
    "    class_path = os.path.join(fasta_path, file_class)\n",
    "\n",
    "    for fasta_file in os.listdir(class_path):\n",
    "        fasta_path_full = os.path.join(class_path, fasta_file)\n",
    "\n",
    "        sequences = list(SeqIO.parse(fasta_path_full, \"fasta\"))\n",
    "\n",
    "        print(f\"\\nProcessing {fasta_file} ({len(sequences)} sequences)\")\n",
    "\n",
    "        for record in tqdm(sequences):\n",
    "            save_path = os.path.join(out_path, f\"{record.id}.npy\")\n",
    "\n",
    "            # Skip if already processed\n",
    "            if os.path.exists(save_path):\n",
    "                continue\n",
    "\n",
    "            seq = str(record.seq)\n",
    "\n",
    "            # Tokenize for HF model\n",
    "            encoded = tokenizer(\n",
    "                seq,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=False,\n",
    "                add_special_tokens=True\n",
    "            )\n",
    "\n",
    "            encoded = {k: v.to(DEVICE) for k, v in encoded.items()}\n",
    "\n",
    "            # Run model on GPU\n",
    "            with torch.no_grad():\n",
    "                output = model(**encoded)\n",
    "                hidden = output.last_hidden_state.cpu()      # shape: (1, L, D)\n",
    "                embedding = hidden[0, 1:len(seq)+1].numpy()\n",
    "\n",
    "            # Save\n",
    "            np.save(save_path, embedding)\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        \n",
    "\n",
    "# Cleanup\n",
    "del model\n",
    "gc.collect()\n",
    "print(\"\\n✓ Done extracting ESM embeddings!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping bad file: 6s9fB.npy Error: Failed to interpret file '/virtual/zengzix4/LABind_ESM/LigBind/esm3B/6s9fB.npy' as a pickle\n",
      "Skipping bad file: 6ptkA.npy Error: cannot reshape array of size 1138656 into shape (471,2560)\n",
      "FINAL ESM MIN SHAPE: (2560,)\n",
      "FINAL ESM MAX SHAPE: (2560,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "esm_dir = \"/virtual/zengzix4/LABind_ESM/LigBind/esm3B\"    # <-- CHANGE THIS\n",
    "save_file = \"esm2_repr_stats.npy\"\n",
    "save_dir = \"/virtual/zengzix4/LABind_ESM/tools/\"\n",
    "# Use None so we can initialize on first file\n",
    "\n",
    "all_min = None\n",
    "all_max = None\n",
    "\n",
    "for f in os.listdir(esm_dir):\n",
    "    if not f.endswith(\".npy\"):\n",
    "        continue\n",
    "\n",
    "    fpath = os.path.join(esm_dir, f)\n",
    "\n",
    "    try:\n",
    "        arr = np.load(fpath, allow_pickle=True)\n",
    "    except Exception as e:\n",
    "        print(\"Skipping bad file:\", f, \"Error:\", e)\n",
    "        continue\n",
    "\n",
    "    # Convert object arrays to real numpy if possible\n",
    "    if isinstance(arr, np.ndarray) and arr.dtype == object:\n",
    "        try:\n",
    "            arr = np.vstack(arr)  # many ESM embeddings come as list of arrays\n",
    "        except:\n",
    "            print(\"Skipping malformed object array:\", f)\n",
    "            continue\n",
    "\n",
    "    # Now check shape\n",
    "    if arr.ndim != 2:\n",
    "        print(\"Skipping wrong shape\", f, arr.shape)\n",
    "        continue\n",
    "\n",
    "    # Compute per-dimension min/max\n",
    "    fmin = arr.min(axis=0)\n",
    "    fmax = arr.max(axis=0)\n",
    "\n",
    "    if all_min is None:\n",
    "        all_min = fmin\n",
    "        all_max = fmax\n",
    "    else:\n",
    "        all_min = np.minimum(all_min, fmin)\n",
    "        all_max = np.maximum(all_max, fmax)\n",
    "\n",
    "print(\"FINAL ESM MIN SHAPE:\", all_min.shape)\n",
    "print(\"FINAL ESM MAX SHAPE:\", all_max.shape)\n",
    "np.save(os.path.join(save_dir, \"esm2_min_repr.npy\"), all_min)\n",
    "np.save(os.path.join(save_dir, \"esm2_max_repr.npy\"), all_max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Ankh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/virtual/zengzix4/miniconda/envs/LABind/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ankh features\n",
    "from transformers import AutoTokenizer, T5EncoderModel \n",
    "from Bio import SeqIO\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrain_path['ankh_path'])\n",
    "model     = T5EncoderModel.from_pretrained(pretrain_path['ankh_path'])\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "out_path = f\"{root_path}/{dataset}/ankh/\"\n",
    "makeDir(out_path)\n",
    "# 使用biopython读取fasta文件\n",
    "fasta_path = f\"{root_path}/{dataset}/fasta/\"\n",
    "for file_class in os.listdir(fasta_path):\n",
    "    for fasta_base_file in os.listdir(f\"{fasta_path}/{file_class}\"):\n",
    "        fasta_file = f\"{fasta_path}/{file_class}/{fasta_base_file}\"\n",
    "        sequences = SeqIO.parse(fasta_file, \"fasta\")\n",
    "        for record in tqdm(sequences):\n",
    "            if os.path.exists(out_path+f'{record.id}.npy'):\n",
    "                continue\n",
    "            ids = tokenizer.batch_encode_plus([list(record.seq)], add_special_tokens=True, padding=True, is_split_into_words=True, return_tensors=\"pt\")\n",
    "            input_ids = ids['input_ids'].to(DEVICE)\n",
    "            attention_mask = ids['attention_mask'].to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                embedding_repr = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "                emb = embedding_repr.last_hidden_state[0,:len(record.seq)].cpu().numpy()\n",
    "                np.save(out_path+f'{record.id}.npy',emb)\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DSSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DSSP running: 100%|███████████████| 11121/11121 [1:22:02<00:00,  2.26proteins/s]\n"
     ]
    }
   ],
   "source": [
    "from Bio.PDB import PDBParser\n",
    "from Bio.PDB.DSSP import DSSP\n",
    "\n",
    "mapSS = {' ':[0,0,0,0,0,0,0,0,0],\n",
    "        '-':[1,0,0,0,0,0,0,0,0],\n",
    "        'H':[0,1,0,0,0,0,0,0,0],\n",
    "        'B':[0,0,1,0,0,0,0,0,0],\n",
    "        'E':[0,0,0,1,0,0,0,0,0],\n",
    "        'G':[0,0,0,0,1,0,0,0,0],\n",
    "        'I':[0,0,0,0,0,1,0,0,0],\n",
    "        'P':[0,0,0,0,0,0,1,0,0],\n",
    "        'T':[0,0,0,0,0,0,0,1,0],\n",
    "        'S':[0,0,0,0,0,0,0,0,1]}\n",
    "p = PDBParser(QUIET=True)\n",
    "pdb_path = f\"{root_path}/{dataset}/pdb/\"\n",
    "dssp_path = \"../tools/mkdssp\"\n",
    "pdb_class = nn_config['pdb_class']\n",
    "makeDir(f\"{root_path}/{dataset}/{pdb_class}_dssp/\")\n",
    "test_files = os.listdir(pdb_path)\n",
    "pdb_files = []\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(pdb_path):\n",
    "    for f in filenames:\n",
    "        if f.endswith('.pdb'):\n",
    "            pdb_files.append(os.path.join(dirpath, f)) \n",
    "            \n",
    "for pdb_file_name in tqdm(pdb_files, desc='DSSP running',ncols=80,unit='proteins'):\n",
    "    pdb_file = pdb_file_name\n",
    "    save_file = pdb_file.replace('.pdb','.npy').replace('pdb',f'{pdb_class}_dssp')\n",
    "    if os.path.exists(save_file):\n",
    "        continue\n",
    "    structure = p.get_structure(\"tmp\", pdb_file)\n",
    "    model = structure[0]\n",
    "    try:\n",
    "        dssp = DSSP(model, pdb_file, dssp=dssp_path)\n",
    "        keys = list(dssp.keys())\n",
    "    except:\n",
    "        keys = []\n",
    "    res_np = []\n",
    "    for chain in model:\n",
    "        for residue in chain:\n",
    "            res_key = (chain.id,(' ', residue.id[1], residue.id[2]))\n",
    "            if res_key in keys:\n",
    "                tuple_dssp = dssp[res_key]\n",
    "                res_np.append(mapSS[tuple_dssp[2]] + list(tuple_dssp[3:]))\n",
    "            else:\n",
    "                res_np.append(np.zeros(20))\n",
    "    os.makedirs(os.path.dirname(save_file), exist_ok=True)\n",
    "    np.save(save_file, np.array(res_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB.ResidueDepth import get_surface\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "pdb_path = f\"{root_path}/{dataset}/pdb/\"\n",
    "msms_path = \"../tools/msms\"\n",
    "pdb_class = nn_config['pdb_class']\n",
    "makeDir(f\"{root_path}/{dataset}/{pdb_class}_pos/\")\n",
    "pdb_files = []\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(pdb_path):\n",
    "    for f in filenames:\n",
    "        if f.endswith('.pdb'):\n",
    "            pdb_files.append(os.path.join(dirpath, f)) \n",
    "\n",
    "for pdb_file in tqdm(pdb_files,desc='MSMS running',ncols=80,unit='proteins'):\n",
    "    save_file = pdb_file.replace('.pdb','.npy').replace('pdb',f'{pdb_class}_pos')\n",
    "    if os.path.exists(save_file):\n",
    "        continue\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    X = []\n",
    "    chain_atom = ['N', 'CA', 'C', 'O']\n",
    "    model = parser.get_structure('model', pdb_file)[0]\n",
    "    chain = next(model.get_chains())\n",
    "    try:\n",
    "        surf = get_surface(chain,MSMS=msms_path)\n",
    "        surf_tree = cKDTree(surf)\n",
    "    except:\n",
    "        surf = np.empty(0)\n",
    "    for residue in chain:\n",
    "        line = []\n",
    "        atoms_coord = np.array([atom.get_coord() for atom in residue])\n",
    "        if surf.size == 0:\n",
    "            dist, _ = surf_tree.query(atoms_coord)\n",
    "            closest_atom = np.argmin(dist)\n",
    "            closest_pos = atoms_coord[closest_atom]\n",
    "        else:\n",
    "            closest_pos = atoms_coord[-1]\n",
    "        atoms = list(residue.get_atoms())\n",
    "        try:\n",
    "            ca_pos = residue['CA'].get_coord()\n",
    "        except KeyError:\n",
    "            print(residue)\n",
    "            continue\n",
    "\n",
    "        pos_s = 0\n",
    "        un_s = 0\n",
    "        for atom in atoms:\n",
    "            if atom.name in chain_atom:\n",
    "                line.append(atom.get_coord())\n",
    "            else:\n",
    "                pos_s += calMass(atom,True)\n",
    "                un_s += calMass(atom,False)\n",
    "        # 此处line应该等于4\n",
    "        if len(line) != 4:\n",
    "            line = line + [list(ca_pos)]*(4-len(line))\n",
    "        if un_s == 0:\n",
    "            R_pos = ca_pos\n",
    "        else:\n",
    "            R_pos = pos_s / un_s\n",
    "        line.append(R_pos)  \n",
    "        line.append(closest_pos) # 加入最近点的残基信息\n",
    "        X.append(line) \n",
    "    \n",
    "    os.makedirs(os.path.dirname(save_file), exist_ok=True)\n",
    "\n",
    "    np.save(save_file, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, valid_list, fold_idx):\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    valid_data = readData.readData(\n",
    "        name_list=valid_list, \n",
    "        proj_dir=nn_config['proj_dir'], \n",
    "        lig_dict=nn_config['lig_dict'],\n",
    "        true_file=nn_config['train_file'], mode='train') # If 5-fold cross-validation is not used, it needs to be changed to valid_file.\n",
    "    valid_loader = DataLoader(valid_data, batch_size=nn_config['batch_size'],shuffle=True, collate_fn=valid_data.collate_fn, num_workers=5)\n",
    "    all_y_score = []\n",
    "    all_y_true = []\n",
    "    with torch.no_grad():\n",
    "        for rfeat, ligand, xyz,  mask, y_true in valid_loader:\n",
    "            tensors = [rfeat, ligand, xyz,  mask, y_true]\n",
    "            tensors = [tensor.to(DEVICE) for tensor in tensors]\n",
    "            rfeat, ligand, xyz, mask, y_true = tensors\n",
    "            logits = model(rfeat, ligand, xyz,  mask).sigmoid() # [N]\n",
    "            logits = torch.masked_select(logits, mask==1)\n",
    "            y_true = torch.masked_select(y_true, mask==1)\n",
    "            all_y_score.extend(logits.cpu().detach().numpy())\n",
    "            all_y_true.extend(y_true.cpu().detach().numpy())\n",
    "        # 通过aupr数值进行早停\n",
    "        aupr_value = average_precision_score(all_y_true, all_y_score)\n",
    "    return aupr_value\n",
    "\n",
    "def train(train_list,valid_list=None,model=None,epochs=50,fold_idx=None):\n",
    "    model.to(DEVICE)\n",
    "    train_data = readData.readData(\n",
    "        name_list=train_list, \n",
    "        proj_dir=nn_config['proj_dir'], \n",
    "        lig_dict=nn_config['lig_dict'],\n",
    "        true_file=nn_config['train_file'], mode='train')\n",
    "    train_loader = DataLoader(train_data, batch_size=nn_config['batch_size'],shuffle=True, collate_fn=train_data.collate_fn, num_workers=5)\n",
    "    loss_fn = nn.BCELoss(reduction='none')\n",
    "    optimizer = get_std_opt(len(train_list),nn_config['batch_size'], model.parameters(), nn_config['hidden_dim'], nn_config['lr'])\n",
    "    v_max_aupr = 0\n",
    "    patience = 0\n",
    "    t_mccs = []\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model,device_ids=nn_config['device_ids'])\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        all_loss = 0\n",
    "        all_cnt = 0\n",
    "        model.train()\n",
    "        for rfeat, ligand, xyz,  mask, y_true in tqdm(train_loader):\n",
    "            tensors = [rfeat, ligand, xyz,  mask, y_true]\n",
    "            tensors = [tensor.to(DEVICE) for tensor in tensors]\n",
    "            rfeat, ligand, xyz, mask, y_true = tensors\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(rfeat, ligand, xyz, mask).sigmoid() # [N]\n",
    "            # 计算所有离子的loss\n",
    "            loss = loss_fn(logits, y_true) * mask\n",
    "            loss = loss.sum() / mask.sum()\n",
    "            all_loss += loss.item()\n",
    "            all_cnt += 1\n",
    "            loss.backward()\n",
    "            # NEW\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        train_losses.append(all_loss / all_cnt)\n",
    "        # 根据验证集的aupr进行早停\n",
    "        if valid_list is not None:\n",
    "            v_aupr = valid(model,valid_list, fold_idx)\n",
    "            t_mccs.append(v_aupr)\n",
    "            print(f'Epoch {epoch} Loss: {all_loss / all_cnt}', f'Epoch Valid {epoch} AUPR: {v_aupr}')\n",
    "            if v_aupr > v_max_aupr:\n",
    "                v_max_aupr = v_aupr\n",
    "                patience = 0\n",
    "                torch.save(model.state_dict(), f'{root_path}/Output/{dataset}_5fold/fold{fold_idx}.ckpt')\n",
    "            else:\n",
    "                patience += 1\n",
    "            if patience >= nn_config['max_patience']:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setALlSeed(11)\n",
    "# 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state = 42)\n",
    "data_list = readDataList(f'{root_path}/{dataset}/label/train/train.fa',skew=1)\n",
    "makeDir(f'{root_path}/Output/{dataset}_5fold/')\n",
    "fold_idx = 0\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "for train_idx, valid_idx in kf.split(data_list):\n",
    "    train_list = [data_list[i] for i in train_idx]\n",
    "    valid_list = [data_list[j] for j in valid_idx]\n",
    "    model = LABind(\n",
    "    rfeat_dim=nn_config['rfeat_dim'], ligand_dim=nn_config['ligand_dim'], hidden_dim=nn_config['hidden_dim'], heads=nn_config['heads'], augment_eps=nn_config['augment_eps'], \n",
    "    rbf_num=nn_config['rbf_num'],top_k=nn_config['top_k'], attn_drop=nn_config['attn_drop'], dropout=nn_config['dropout'], num_layers=nn_config['num_layers'])\n",
    "    train(train_list,valid_list,model,epochs=70,fold_idx=fold_idx)\n",
    "    fold_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 7452/7452 [06:18<00:00, 19.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 0.10398311041835676 Epoch Valid 0 AUPR: 0.35387015061608573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 7452/7452 [06:19<00:00, 19.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.0786849736162089 Epoch Valid 1 AUPR: 0.40323699306164873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 7452/7452 [06:19<00:00, 19.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.07507787457394156 Epoch Valid 2 AUPR: 0.4754686796474941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▌                                      | 286/7452 [00:14<06:04, 19.67it/s]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "all_data = readDataList(f'{root_path}/{dataset}/label/train/train.fa', skew=1) \n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "train_list, valid_list = train_test_split( ## no given validation in LigBind\n",
    "    all_data,\n",
    "    test_size=0.1,       # 10% validation\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "makeDir(f'{root_path}/Output/{dataset}/')\n",
    "model = LABind(\n",
    "rfeat_dim=nn_config['rfeat_dim'], ligand_dim=nn_config['ligand_dim'], hidden_dim=nn_config['hidden_dim'], heads=nn_config['heads'], augment_eps=nn_config['augment_eps'], \n",
    "rbf_num=nn_config['rbf_num'],top_k=nn_config['top_k'], attn_drop=nn_config['attn_drop'], dropout=nn_config['dropout'], num_layers=nn_config['num_layers'])\n",
    "train(train_list, valid_list, model, epochs=50, fold_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/virtual/zengzix4/LABind_ESM/Output/LigBind_5fold/\n",
      "source\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for DataParallel:\n\tMissing key(s) in state_dict: \"module.in_mlp.fc1.weight\", \"module.in_mlp.fc1.bias\", \"module.in_mlp.fc2.weight\", \"module.in_mlp.fc2.bias\", \"module.in_mlp.ln1.weight\", \"module.in_mlp.ln1.bias\", \"module.in_mlp.ln2.weight\", \"module.in_mlp.ln2.bias\", \"module.lig_mlp.fc1.0.weight\", \"module.lig_mlp.fc1.0.bias\", \"module.lig_mlp.fc1.1.weight\", \"module.lig_mlp.fc1.1.bias\", \"module.edge_feature.edge_emb.weight\", \"module.edge_feature.edge_emb.bias\", \"module.edge_feature.norm_edge.weight\", \"module.edge_feature.norm_edge.bias\", \"module.edge_feature.node_emb.weight\", \"module.edge_feature.node_emb.bias\", \"module.edge_feature.norm_node.weight\", \"module.edge_feature.norm_node.bias\", \"module.f_mlp.0.weight\", \"module.f_mlp.0.bias\", \"module.f_mlp.2.weight\", \"module.f_mlp.2.bias\", \"module.f_mlp.4.weight\", \"module.f_mlp.4.bias\", \"module.conv_layers.0.norm.0.weight\", \"module.conv_layers.0.norm.0.bias\", \"module.conv_layers.0.norm.1.weight\", \"module.conv_layers.0.norm.1.bias\", \"module.conv_layers.0.norm.2.weight\", \"module.conv_layers.0.norm.2.bias\", \"module.conv_layers.0.CrossAttn.query.weight\", \"module.conv_layers.0.CrossAttn.query.bias\", \"module.conv_layers.0.CrossAttn.key.weight\", \"module.conv_layers.0.CrossAttn.key.bias\", \"module.conv_layers.0.CrossAttn.value.weight\", \"module.conv_layers.0.CrossAttn.value.bias\", \"module.conv_layers.0.CrossAttn.out.weight\", \"module.conv_layers.0.CrossAttn.out.bias\", \"module.conv_layers.0.NeighAttn.W_Q.weight\", \"module.conv_layers.0.NeighAttn.W_K.weight\", \"module.conv_layers.0.NeighAttn.W_V.weight\", \"module.conv_layers.0.NeighAttn.W_O.weight\", \"module.conv_layers.0.dense.linear1.weight\", \"module.conv_layers.0.dense.linear1.bias\", \"module.conv_layers.0.dense.linear2.weight\", \"module.conv_layers.0.dense.linear2.bias\", \"module.conv_layers.0.edge_update.norm.weight\", \"module.conv_layers.0.edge_update.norm.bias\", \"module.conv_layers.0.edge_update.EdgeMLP.0.weight\", \"module.conv_layers.0.edge_update.EdgeMLP.0.bias\", \"module.conv_layers.0.edge_update.EdgeMLP.2.weight\", \"module.conv_layers.0.edge_update.EdgeMLP.2.bias\", \"module.conv_layers.0.context.ContextMLP.0.weight\", \"module.conv_layers.0.context.ContextMLP.0.bias\", \"module.conv_layers.0.context.ContextMLP.2.weight\", \"module.conv_layers.0.context.ContextMLP.2.bias\", \"module.conv_layers.0.context.norm.weight\", \"module.conv_layers.0.context.norm.bias\", \"module.conv_layers.1.norm.0.weight\", \"module.conv_layers.1.norm.0.bias\", \"module.conv_layers.1.norm.1.weight\", \"module.conv_layers.1.norm.1.bias\", \"module.conv_layers.1.norm.2.weight\", \"module.conv_layers.1.norm.2.bias\", \"module.conv_layers.1.CrossAttn.query.weight\", \"module.conv_layers.1.CrossAttn.query.bias\", \"module.conv_layers.1.CrossAttn.key.weight\", \"module.conv_layers.1.CrossAttn.key.bias\", \"module.conv_layers.1.CrossAttn.value.weight\", \"module.conv_layers.1.CrossAttn.value.bias\", \"module.conv_layers.1.CrossAttn.out.weight\", \"module.conv_layers.1.CrossAttn.out.bias\", \"module.conv_layers.1.NeighAttn.W_Q.weight\", \"module.conv_layers.1.NeighAttn.W_K.weight\", \"module.conv_layers.1.NeighAttn.W_V.weight\", \"module.conv_layers.1.NeighAttn.W_O.weight\", \"module.conv_layers.1.dense.linear1.weight\", \"module.conv_layers.1.dense.linear1.bias\", \"module.conv_layers.1.dense.linear2.weight\", \"module.conv_layers.1.dense.linear2.bias\", \"module.conv_layers.1.edge_update.norm.weight\", \"module.conv_layers.1.edge_update.norm.bias\", \"module.conv_layers.1.edge_update.EdgeMLP.0.weight\", \"module.conv_layers.1.edge_update.EdgeMLP.0.bias\", \"module.conv_layers.1.edge_update.EdgeMLP.2.weight\", \"module.conv_layers.1.edge_update.EdgeMLP.2.bias\", \"module.conv_layers.1.context.ContextMLP.0.weight\", \"module.conv_layers.1.context.ContextMLP.0.bias\", \"module.conv_layers.1.context.ContextMLP.2.weight\", \"module.conv_layers.1.context.ContextMLP.2.bias\", \"module.conv_layers.1.context.norm.weight\", \"module.conv_layers.1.context.norm.bias\", \"module.conv_layers.2.norm.0.weight\", \"module.conv_layers.2.norm.0.bias\", \"module.conv_layers.2.norm.1.weight\", \"module.conv_layers.2.norm.1.bias\", \"module.conv_layers.2.norm.2.weight\", \"module.conv_layers.2.norm.2.bias\", \"module.conv_layers.2.CrossAttn.query.weight\", \"module.conv_layers.2.CrossAttn.query.bias\", \"module.conv_layers.2.CrossAttn.key.weight\", \"module.conv_layers.2.CrossAttn.key.bias\", \"module.conv_layers.2.CrossAttn.value.weight\", \"module.conv_layers.2.CrossAttn.value.bias\", \"module.conv_layers.2.CrossAttn.out.weight\", \"module.conv_layers.2.CrossAttn.out.bias\", \"module.conv_layers.2.NeighAttn.W_Q.weight\", \"module.conv_layers.2.NeighAttn.W_K.weight\", \"module.conv_layers.2.NeighAttn.W_V.weight\", \"module.conv_layers.2.NeighAttn.W_O.weight\", \"module.conv_layers.2.dense.linear1.weight\", \"module.conv_layers.2.dense.linear1.bias\", \"module.conv_layers.2.dense.linear2.weight\", \"module.conv_layers.2.dense.linear2.bias\", \"module.conv_layers.2.edge_update.norm.weight\", \"module.conv_layers.2.edge_update.norm.bias\", \"module.conv_layers.2.edge_update.EdgeMLP.0.weight\", \"module.conv_layers.2.edge_update.EdgeMLP.0.bias\", \"module.conv_layers.2.edge_update.EdgeMLP.2.weight\", \"module.conv_layers.2.edge_update.EdgeMLP.2.bias\", \"module.conv_layers.2.context.ContextMLP.0.weight\", \"module.conv_layers.2.context.ContextMLP.0.bias\", \"module.conv_layers.2.context.ContextMLP.2.weight\", \"module.conv_layers.2.context.ContextMLP.2.bias\", \"module.conv_layers.2.context.norm.weight\", \"module.conv_layers.2.context.norm.bias\", \"module.conv_layers.3.norm.0.weight\", \"module.conv_layers.3.norm.0.bias\", \"module.conv_layers.3.norm.1.weight\", \"module.conv_layers.3.norm.1.bias\", \"module.conv_layers.3.norm.2.weight\", \"module.conv_layers.3.norm.2.bias\", \"module.conv_layers.3.CrossAttn.query.weight\", \"module.conv_layers.3.CrossAttn.query.bias\", \"module.conv_layers.3.CrossAttn.key.weight\", \"module.conv_layers.3.CrossAttn.key.bias\", \"module.conv_layers.3.CrossAttn.value.weight\", \"module.conv_layers.3.CrossAttn.value.bias\", \"module.conv_layers.3.CrossAttn.out.weight\", \"module.conv_layers.3.CrossAttn.out.bias\", \"module.conv_layers.3.NeighAttn.W_Q.weight\", \"module.conv_layers.3.NeighAttn.W_K.weight\", \"module.conv_layers.3.NeighAttn.W_V.weight\", \"module.conv_layers.3.NeighAttn.W_O.weight\", \"module.conv_layers.3.dense.linear1.weight\", \"module.conv_layers.3.dense.linear1.bias\", \"module.conv_layers.3.dense.linear2.weight\", \"module.conv_layers.3.dense.linear2.bias\", \"module.conv_layers.3.edge_update.norm.weight\", \"module.conv_layers.3.edge_update.norm.bias\", \"module.conv_layers.3.edge_update.EdgeMLP.0.weight\", \"module.conv_layers.3.edge_update.EdgeMLP.0.bias\", \"module.conv_layers.3.edge_update.EdgeMLP.2.weight\", \"module.conv_layers.3.edge_update.EdgeMLP.2.bias\", \"module.conv_layers.3.context.ContextMLP.0.weight\", \"module.conv_layers.3.context.ContextMLP.0.bias\", \"module.conv_layers.3.context.ContextMLP.2.weight\", \"module.conv_layers.3.context.ContextMLP.2.bias\", \"module.conv_layers.3.context.norm.weight\", \"module.conv_layers.3.context.norm.bias\", \"module.out_mlp.0.weight\", \"module.out_mlp.0.bias\", \"module.out_mlp.2.weight\", \"module.out_mlp.2.bias\". \n\tUnexpected key(s) in state_dict: \"in_mlp.fc1.weight\", \"in_mlp.fc1.bias\", \"in_mlp.fc2.weight\", \"in_mlp.fc2.bias\", \"in_mlp.ln1.weight\", \"in_mlp.ln1.bias\", \"in_mlp.ln2.weight\", \"in_mlp.ln2.bias\", \"lig_mlp.fc1.0.weight\", \"lig_mlp.fc1.0.bias\", \"lig_mlp.fc1.1.weight\", \"lig_mlp.fc1.1.bias\", \"edge_feature.edge_emb.weight\", \"edge_feature.edge_emb.bias\", \"edge_feature.norm_edge.weight\", \"edge_feature.norm_edge.bias\", \"edge_feature.node_emb.weight\", \"edge_feature.node_emb.bias\", \"edge_feature.norm_node.weight\", \"edge_feature.norm_node.bias\", \"f_mlp.0.weight\", \"f_mlp.0.bias\", \"f_mlp.2.weight\", \"f_mlp.2.bias\", \"f_mlp.4.weight\", \"f_mlp.4.bias\", \"conv_layers.0.norm.0.weight\", \"conv_layers.0.norm.0.bias\", \"conv_layers.0.norm.1.weight\", \"conv_layers.0.norm.1.bias\", \"conv_layers.0.norm.2.weight\", \"conv_layers.0.norm.2.bias\", \"conv_layers.0.CrossAttn.query.weight\", \"conv_layers.0.CrossAttn.query.bias\", \"conv_layers.0.CrossAttn.key.weight\", \"conv_layers.0.CrossAttn.key.bias\", \"conv_layers.0.CrossAttn.value.weight\", \"conv_layers.0.CrossAttn.value.bias\", \"conv_layers.0.CrossAttn.out.weight\", \"conv_layers.0.CrossAttn.out.bias\", \"conv_layers.0.NeighAttn.W_Q.weight\", \"conv_layers.0.NeighAttn.W_K.weight\", \"conv_layers.0.NeighAttn.W_V.weight\", \"conv_layers.0.NeighAttn.W_O.weight\", \"conv_layers.0.dense.linear1.weight\", \"conv_layers.0.dense.linear1.bias\", \"conv_layers.0.dense.linear2.weight\", \"conv_layers.0.dense.linear2.bias\", \"conv_layers.0.edge_update.norm.weight\", \"conv_layers.0.edge_update.norm.bias\", \"conv_layers.0.edge_update.EdgeMLP.0.weight\", \"conv_layers.0.edge_update.EdgeMLP.0.bias\", \"conv_layers.0.edge_update.EdgeMLP.2.weight\", \"conv_layers.0.edge_update.EdgeMLP.2.bias\", \"conv_layers.0.context.ContextMLP.0.weight\", \"conv_layers.0.context.ContextMLP.0.bias\", \"conv_layers.0.context.ContextMLP.2.weight\", \"conv_layers.0.context.ContextMLP.2.bias\", \"conv_layers.0.context.norm.weight\", \"conv_layers.0.context.norm.bias\", \"conv_layers.1.norm.0.weight\", \"conv_layers.1.norm.0.bias\", \"conv_layers.1.norm.1.weight\", \"conv_layers.1.norm.1.bias\", \"conv_layers.1.norm.2.weight\", \"conv_layers.1.norm.2.bias\", \"conv_layers.1.CrossAttn.query.weight\", \"conv_layers.1.CrossAttn.query.bias\", \"conv_layers.1.CrossAttn.key.weight\", \"conv_layers.1.CrossAttn.key.bias\", \"conv_layers.1.CrossAttn.value.weight\", \"conv_layers.1.CrossAttn.value.bias\", \"conv_layers.1.CrossAttn.out.weight\", \"conv_layers.1.CrossAttn.out.bias\", \"conv_layers.1.NeighAttn.W_Q.weight\", \"conv_layers.1.NeighAttn.W_K.weight\", \"conv_layers.1.NeighAttn.W_V.weight\", \"conv_layers.1.NeighAttn.W_O.weight\", \"conv_layers.1.dense.linear1.weight\", \"conv_layers.1.dense.linear1.bias\", \"conv_layers.1.dense.linear2.weight\", \"conv_layers.1.dense.linear2.bias\", \"conv_layers.1.edge_update.norm.weight\", \"conv_layers.1.edge_update.norm.bias\", \"conv_layers.1.edge_update.EdgeMLP.0.weight\", \"conv_layers.1.edge_update.EdgeMLP.0.bias\", \"conv_layers.1.edge_update.EdgeMLP.2.weight\", \"conv_layers.1.edge_update.EdgeMLP.2.bias\", \"conv_layers.1.context.ContextMLP.0.weight\", \"conv_layers.1.context.ContextMLP.0.bias\", \"conv_layers.1.context.ContextMLP.2.weight\", \"conv_layers.1.context.ContextMLP.2.bias\", \"conv_layers.1.context.norm.weight\", \"conv_layers.1.context.norm.bias\", \"conv_layers.2.norm.0.weight\", \"conv_layers.2.norm.0.bias\", \"conv_layers.2.norm.1.weight\", \"conv_layers.2.norm.1.bias\", \"conv_layers.2.norm.2.weight\", \"conv_layers.2.norm.2.bias\", \"conv_layers.2.CrossAttn.query.weight\", \"conv_layers.2.CrossAttn.query.bias\", \"conv_layers.2.CrossAttn.key.weight\", \"conv_layers.2.CrossAttn.key.bias\", \"conv_layers.2.CrossAttn.value.weight\", \"conv_layers.2.CrossAttn.value.bias\", \"conv_layers.2.CrossAttn.out.weight\", \"conv_layers.2.CrossAttn.out.bias\", \"conv_layers.2.NeighAttn.W_Q.weight\", \"conv_layers.2.NeighAttn.W_K.weight\", \"conv_layers.2.NeighAttn.W_V.weight\", \"conv_layers.2.NeighAttn.W_O.weight\", \"conv_layers.2.dense.linear1.weight\", \"conv_layers.2.dense.linear1.bias\", \"conv_layers.2.dense.linear2.weight\", \"conv_layers.2.dense.linear2.bias\", \"conv_layers.2.edge_update.norm.weight\", \"conv_layers.2.edge_update.norm.bias\", \"conv_layers.2.edge_update.EdgeMLP.0.weight\", \"conv_layers.2.edge_update.EdgeMLP.0.bias\", \"conv_layers.2.edge_update.EdgeMLP.2.weight\", \"conv_layers.2.edge_update.EdgeMLP.2.bias\", \"conv_layers.2.context.ContextMLP.0.weight\", \"conv_layers.2.context.ContextMLP.0.bias\", \"conv_layers.2.context.ContextMLP.2.weight\", \"conv_layers.2.context.ContextMLP.2.bias\", \"conv_layers.2.context.norm.weight\", \"conv_layers.2.context.norm.bias\", \"conv_layers.3.norm.0.weight\", \"conv_layers.3.norm.0.bias\", \"conv_layers.3.norm.1.weight\", \"conv_layers.3.norm.1.bias\", \"conv_layers.3.norm.2.weight\", \"conv_layers.3.norm.2.bias\", \"conv_layers.3.CrossAttn.query.weight\", \"conv_layers.3.CrossAttn.query.bias\", \"conv_layers.3.CrossAttn.key.weight\", \"conv_layers.3.CrossAttn.key.bias\", \"conv_layers.3.CrossAttn.value.weight\", \"conv_layers.3.CrossAttn.value.bias\", \"conv_layers.3.CrossAttn.out.weight\", \"conv_layers.3.CrossAttn.out.bias\", \"conv_layers.3.NeighAttn.W_Q.weight\", \"conv_layers.3.NeighAttn.W_K.weight\", \"conv_layers.3.NeighAttn.W_V.weight\", \"conv_layers.3.NeighAttn.W_O.weight\", \"conv_layers.3.dense.linear1.weight\", \"conv_layers.3.dense.linear1.bias\", \"conv_layers.3.dense.linear2.weight\", \"conv_layers.3.dense.linear2.bias\", \"conv_layers.3.edge_update.norm.weight\", \"conv_layers.3.edge_update.norm.bias\", \"conv_layers.3.edge_update.EdgeMLP.0.weight\", \"conv_layers.3.edge_update.EdgeMLP.0.bias\", \"conv_layers.3.edge_update.EdgeMLP.2.weight\", \"conv_layers.3.edge_update.EdgeMLP.2.bias\", \"conv_layers.3.context.ContextMLP.0.weight\", \"conv_layers.3.context.ContextMLP.0.bias\", \"conv_layers.3.context.ContextMLP.2.weight\", \"conv_layers.3.context.ContextMLP.2.bias\", \"conv_layers.3.context.norm.weight\", \"conv_layers.3.context.norm.bias\", \"out_mlp.0.weight\", \"out_mlp.0.bias\", \"out_mlp.2.weight\", \"out_mlp.2.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m LABind(\n\u001b[1;32m     10\u001b[0m     rfeat_dim\u001b[38;5;241m=\u001b[39mnn_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrfeat_dim\u001b[39m\u001b[38;5;124m'\u001b[39m], ligand_dim\u001b[38;5;241m=\u001b[39mnn_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mligand_dim\u001b[39m\u001b[38;5;124m'\u001b[39m], hidden_dim\u001b[38;5;241m=\u001b[39mnn_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m], heads\u001b[38;5;241m=\u001b[39mnn_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheads\u001b[39m\u001b[38;5;124m'\u001b[39m], augment_eps\u001b[38;5;241m=\u001b[39mnn_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maugment_eps\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     11\u001b[0m     rbf_num\u001b[38;5;241m=\u001b[39mnn_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrbf_num\u001b[39m\u001b[38;5;124m'\u001b[39m],top_k\u001b[38;5;241m=\u001b[39mnn_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m'\u001b[39m], attn_drop\u001b[38;5;241m=\u001b[39mnn_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattn_drop\u001b[39m\u001b[38;5;124m'\u001b[39m], dropout\u001b[38;5;241m=\u001b[39mnn_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m'\u001b[39m], num_layers\u001b[38;5;241m=\u001b[39mnn_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDataParallel(model,device_ids\u001b[38;5;241m=\u001b[39mnn_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 13\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     15\u001b[0m models\u001b[38;5;241m.\u001b[39mappend(model)\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/nn/modules/module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DataParallel:\n\tMissing key(s) in state_dict: \"module.in_mlp.fc1.weight\", \"module.in_mlp.fc1.bias\", \"module.in_mlp.fc2.weight\", \"module.in_mlp.fc2.bias\", \"module.in_mlp.ln1.weight\", \"module.in_mlp.ln1.bias\", \"module.in_mlp.ln2.weight\", \"module.in_mlp.ln2.bias\", \"module.lig_mlp.fc1.0.weight\", \"module.lig_mlp.fc1.0.bias\", \"module.lig_mlp.fc1.1.weight\", \"module.lig_mlp.fc1.1.bias\", \"module.edge_feature.edge_emb.weight\", \"module.edge_feature.edge_emb.bias\", \"module.edge_feature.norm_edge.weight\", \"module.edge_feature.norm_edge.bias\", \"module.edge_feature.node_emb.weight\", \"module.edge_feature.node_emb.bias\", \"module.edge_feature.norm_node.weight\", \"module.edge_feature.norm_node.bias\", \"module.f_mlp.0.weight\", \"module.f_mlp.0.bias\", \"module.f_mlp.2.weight\", \"module.f_mlp.2.bias\", \"module.f_mlp.4.weight\", \"module.f_mlp.4.bias\", \"module.conv_layers.0.norm.0.weight\", \"module.conv_layers.0.norm.0.bias\", \"module.conv_layers.0.norm.1.weight\", \"module.conv_layers.0.norm.1.bias\", \"module.conv_layers.0.norm.2.weight\", \"module.conv_layers.0.norm.2.bias\", \"module.conv_layers.0.CrossAttn.query.weight\", \"module.conv_layers.0.CrossAttn.query.bias\", \"module.conv_layers.0.CrossAttn.key.weight\", \"module.conv_layers.0.CrossAttn.key.bias\", \"module.conv_layers.0.CrossAttn.value.weight\", \"module.conv_layers.0.CrossAttn.value.bias\", \"module.conv_layers.0.CrossAttn.out.weight\", \"module.conv_layers.0.CrossAttn.out.bias\", \"module.conv_layers.0.NeighAttn.W_Q.weight\", \"module.conv_layers.0.NeighAttn.W_K.weight\", \"module.conv_layers.0.NeighAttn.W_V.weight\", \"module.conv_layers.0.NeighAttn.W_O.weight\", \"module.conv_layers.0.dense.linear1.weight\", \"module.conv_layers.0.dense.linear1.bias\", \"module.conv_layers.0.dense.linear2.weight\", \"module.conv_layers.0.dense.linear2.bias\", \"module.conv_layers.0.edge_update.norm.weight\", \"module.conv_layers.0.edge_update.norm.bias\", \"module.conv_layers.0.edge_update.EdgeMLP.0.weight\", \"module.conv_layers.0.edge_update.EdgeMLP.0.bias\", \"module.conv_layers.0.edge_update.EdgeMLP.2.weight\", \"module.conv_layers.0.edge_update.EdgeMLP.2.bias\", \"module.conv_layers.0.context.ContextMLP.0.weight\", \"module.conv_layers.0.context.ContextMLP.0.bias\", \"module.conv_layers.0.context.ContextMLP.2.weight\", \"module.conv_layers.0.context.ContextMLP.2.bias\", \"module.conv_layers.0.context.norm.weight\", \"module.conv_layers.0.context.norm.bias\", \"module.conv_layers.1.norm.0.weight\", \"module.conv_layers.1.norm.0.bias\", \"module.conv_layers.1.norm.1.weight\", \"module.conv_layers.1.norm.1.bias\", \"module.conv_layers.1.norm.2.weight\", \"module.conv_layers.1.norm.2.bias\", \"module.conv_layers.1.CrossAttn.query.weight\", \"module.conv_layers.1.CrossAttn.query.bias\", \"module.conv_layers.1.CrossAttn.key.weight\", \"module.conv_layers.1.CrossAttn.key.bias\", \"module.conv_layers.1.CrossAttn.value.weight\", \"module.conv_layers.1.CrossAttn.value.bias\", \"module.conv_layers.1.CrossAttn.out.weight\", \"module.conv_layers.1.CrossAttn.out.bias\", \"module.conv_layers.1.NeighAttn.W_Q.weight\", \"module.conv_layers.1.NeighAttn.W_K.weight\", \"module.conv_layers.1.NeighAttn.W_V.weight\", \"module.conv_layers.1.NeighAttn.W_O.weight\", \"module.conv_layers.1.dense.linear1.weight\", \"module.conv_layers.1.dense.linear1.bias\", \"module.conv_layers.1.dense.linear2.weight\", \"module.conv_layers.1.dense.linear2.bias\", \"module.conv_layers.1.edge_update.norm.weight\", \"module.conv_layers.1.edge_update.norm.bias\", \"module.conv_layers.1.edge_update.EdgeMLP.0.weight\", \"module.conv_layers.1.edge_update.EdgeMLP.0.bias\", \"module.conv_layers.1.edge_update.EdgeMLP.2.weight\", \"module.conv_layers.1.edge_update.EdgeMLP.2.bias\", \"module.conv_layers.1.context.ContextMLP.0.weight\", \"module.conv_layers.1.context.ContextMLP.0.bias\", \"module.conv_layers.1.context.ContextMLP.2.weight\", \"module.conv_layers.1.context.ContextMLP.2.bias\", \"module.conv_layers.1.context.norm.weight\", \"module.conv_layers.1.context.norm.bias\", \"module.conv_layers.2.norm.0.weight\", \"module.conv_layers.2.norm.0.bias\", \"module.conv_layers.2.norm.1.weight\", \"module.conv_layers.2.norm.1.bias\", \"module.conv_layers.2.norm.2.weight\", \"module.conv_layers.2.norm.2.bias\", \"module.conv_layers.2.CrossAttn.query.weight\", \"module.conv_layers.2.CrossAttn.query.bias\", \"module.conv_layers.2.CrossAttn.key.weight\", \"module.conv_layers.2.CrossAttn.key.bias\", \"module.conv_layers.2.CrossAttn.value.weight\", \"module.conv_layers.2.CrossAttn.value.bias\", \"module.conv_layers.2.CrossAttn.out.weight\", \"module.conv_layers.2.CrossAttn.out.bias\", \"module.conv_layers.2.NeighAttn.W_Q.weight\", \"module.conv_layers.2.NeighAttn.W_K.weight\", \"module.conv_layers.2.NeighAttn.W_V.weight\", \"module.conv_layers.2.NeighAttn.W_O.weight\", \"module.conv_layers.2.dense.linear1.weight\", \"module.conv_layers.2.dense.linear1.bias\", \"module.conv_layers.2.dense.linear2.weight\", \"module.conv_layers.2.dense.linear2.bias\", \"module.conv_layers.2.edge_update.norm.weight\", \"module.conv_layers.2.edge_update.norm.bias\", \"module.conv_layers.2.edge_update.EdgeMLP.0.weight\", \"module.conv_layers.2.edge_update.EdgeMLP.0.bias\", \"module.conv_layers.2.edge_update.EdgeMLP.2.weight\", \"module.conv_layers.2.edge_update.EdgeMLP.2.bias\", \"module.conv_layers.2.context.ContextMLP.0.weight\", \"module.conv_layers.2.context.ContextMLP.0.bias\", \"module.conv_layers.2.context.ContextMLP.2.weight\", \"module.conv_layers.2.context.ContextMLP.2.bias\", \"module.conv_layers.2.context.norm.weight\", \"module.conv_layers.2.context.norm.bias\", \"module.conv_layers.3.norm.0.weight\", \"module.conv_layers.3.norm.0.bias\", \"module.conv_layers.3.norm.1.weight\", \"module.conv_layers.3.norm.1.bias\", \"module.conv_layers.3.norm.2.weight\", \"module.conv_layers.3.norm.2.bias\", \"module.conv_layers.3.CrossAttn.query.weight\", \"module.conv_layers.3.CrossAttn.query.bias\", \"module.conv_layers.3.CrossAttn.key.weight\", \"module.conv_layers.3.CrossAttn.key.bias\", \"module.conv_layers.3.CrossAttn.value.weight\", \"module.conv_layers.3.CrossAttn.value.bias\", \"module.conv_layers.3.CrossAttn.out.weight\", \"module.conv_layers.3.CrossAttn.out.bias\", \"module.conv_layers.3.NeighAttn.W_Q.weight\", \"module.conv_layers.3.NeighAttn.W_K.weight\", \"module.conv_layers.3.NeighAttn.W_V.weight\", \"module.conv_layers.3.NeighAttn.W_O.weight\", \"module.conv_layers.3.dense.linear1.weight\", \"module.conv_layers.3.dense.linear1.bias\", \"module.conv_layers.3.dense.linear2.weight\", \"module.conv_layers.3.dense.linear2.bias\", \"module.conv_layers.3.edge_update.norm.weight\", \"module.conv_layers.3.edge_update.norm.bias\", \"module.conv_layers.3.edge_update.EdgeMLP.0.weight\", \"module.conv_layers.3.edge_update.EdgeMLP.0.bias\", \"module.conv_layers.3.edge_update.EdgeMLP.2.weight\", \"module.conv_layers.3.edge_update.EdgeMLP.2.bias\", \"module.conv_layers.3.context.ContextMLP.0.weight\", \"module.conv_layers.3.context.ContextMLP.0.bias\", \"module.conv_layers.3.context.ContextMLP.2.weight\", \"module.conv_layers.3.context.ContextMLP.2.bias\", \"module.conv_layers.3.context.norm.weight\", \"module.conv_layers.3.context.norm.bias\", \"module.out_mlp.0.weight\", \"module.out_mlp.0.bias\", \"module.out_mlp.2.weight\", \"module.out_mlp.2.bias\". \n\tUnexpected key(s) in state_dict: \"in_mlp.fc1.weight\", \"in_mlp.fc1.bias\", \"in_mlp.fc2.weight\", \"in_mlp.fc2.bias\", \"in_mlp.ln1.weight\", \"in_mlp.ln1.bias\", \"in_mlp.ln2.weight\", \"in_mlp.ln2.bias\", \"lig_mlp.fc1.0.weight\", \"lig_mlp.fc1.0.bias\", \"lig_mlp.fc1.1.weight\", \"lig_mlp.fc1.1.bias\", \"edge_feature.edge_emb.weight\", \"edge_feature.edge_emb.bias\", \"edge_feature.norm_edge.weight\", \"edge_feature.norm_edge.bias\", \"edge_feature.node_emb.weight\", \"edge_feature.node_emb.bias\", \"edge_feature.norm_node.weight\", \"edge_feature.norm_node.bias\", \"f_mlp.0.weight\", \"f_mlp.0.bias\", \"f_mlp.2.weight\", \"f_mlp.2.bias\", \"f_mlp.4.weight\", \"f_mlp.4.bias\", \"conv_layers.0.norm.0.weight\", \"conv_layers.0.norm.0.bias\", \"conv_layers.0.norm.1.weight\", \"conv_layers.0.norm.1.bias\", \"conv_layers.0.norm.2.weight\", \"conv_layers.0.norm.2.bias\", \"conv_layers.0.CrossAttn.query.weight\", \"conv_layers.0.CrossAttn.query.bias\", \"conv_layers.0.CrossAttn.key.weight\", \"conv_layers.0.CrossAttn.key.bias\", \"conv_layers.0.CrossAttn.value.weight\", \"conv_layers.0.CrossAttn.value.bias\", \"conv_layers.0.CrossAttn.out.weight\", \"conv_layers.0.CrossAttn.out.bias\", \"conv_layers.0.NeighAttn.W_Q.weight\", \"conv_layers.0.NeighAttn.W_K.weight\", \"conv_layers.0.NeighAttn.W_V.weight\", \"conv_layers.0.NeighAttn.W_O.weight\", \"conv_layers.0.dense.linear1.weight\", \"conv_layers.0.dense.linear1.bias\", \"conv_layers.0.dense.linear2.weight\", \"conv_layers.0.dense.linear2.bias\", \"conv_layers.0.edge_update.norm.weight\", \"conv_layers.0.edge_update.norm.bias\", \"conv_layers.0.edge_update.EdgeMLP.0.weight\", \"conv_layers.0.edge_update.EdgeMLP.0.bias\", \"conv_layers.0.edge_update.EdgeMLP.2.weight\", \"conv_layers.0.edge_update.EdgeMLP.2.bias\", \"conv_layers.0.context.ContextMLP.0.weight\", \"conv_layers.0.context.ContextMLP.0.bias\", \"conv_layers.0.context.ContextMLP.2.weight\", \"conv_layers.0.context.ContextMLP.2.bias\", \"conv_layers.0.context.norm.weight\", \"conv_layers.0.context.norm.bias\", \"conv_layers.1.norm.0.weight\", \"conv_layers.1.norm.0.bias\", \"conv_layers.1.norm.1.weight\", \"conv_layers.1.norm.1.bias\", \"conv_layers.1.norm.2.weight\", \"conv_layers.1.norm.2.bias\", \"conv_layers.1.CrossAttn.query.weight\", \"conv_layers.1.CrossAttn.query.bias\", \"conv_layers.1.CrossAttn.key.weight\", \"conv_layers.1.CrossAttn.key.bias\", \"conv_layers.1.CrossAttn.value.weight\", \"conv_layers.1.CrossAttn.value.bias\", \"conv_layers.1.CrossAttn.out.weight\", \"conv_layers.1.CrossAttn.out.bias\", \"conv_layers.1.NeighAttn.W_Q.weight\", \"conv_layers.1.NeighAttn.W_K.weight\", \"conv_layers.1.NeighAttn.W_V.weight\", \"conv_layers.1.NeighAttn.W_O.weight\", \"conv_layers.1.dense.linear1.weight\", \"conv_layers.1.dense.linear1.bias\", \"conv_layers.1.dense.linear2.weight\", \"conv_layers.1.dense.linear2.bias\", \"conv_layers.1.edge_update.norm.weight\", \"conv_layers.1.edge_update.norm.bias\", \"conv_layers.1.edge_update.EdgeMLP.0.weight\", \"conv_layers.1.edge_update.EdgeMLP.0.bias\", \"conv_layers.1.edge_update.EdgeMLP.2.weight\", \"conv_layers.1.edge_update.EdgeMLP.2.bias\", \"conv_layers.1.context.ContextMLP.0.weight\", \"conv_layers.1.context.ContextMLP.0.bias\", \"conv_layers.1.context.ContextMLP.2.weight\", \"conv_layers.1.context.ContextMLP.2.bias\", \"conv_layers.1.context.norm.weight\", \"conv_layers.1.context.norm.bias\", \"conv_layers.2.norm.0.weight\", \"conv_layers.2.norm.0.bias\", \"conv_layers.2.norm.1.weight\", \"conv_layers.2.norm.1.bias\", \"conv_layers.2.norm.2.weight\", \"conv_layers.2.norm.2.bias\", \"conv_layers.2.CrossAttn.query.weight\", \"conv_layers.2.CrossAttn.query.bias\", \"conv_layers.2.CrossAttn.key.weight\", \"conv_layers.2.CrossAttn.key.bias\", \"conv_layers.2.CrossAttn.value.weight\", \"conv_layers.2.CrossAttn.value.bias\", \"conv_layers.2.CrossAttn.out.weight\", \"conv_layers.2.CrossAttn.out.bias\", \"conv_layers.2.NeighAttn.W_Q.weight\", \"conv_layers.2.NeighAttn.W_K.weight\", \"conv_layers.2.NeighAttn.W_V.weight\", \"conv_layers.2.NeighAttn.W_O.weight\", \"conv_layers.2.dense.linear1.weight\", \"conv_layers.2.dense.linear1.bias\", \"conv_layers.2.dense.linear2.weight\", \"conv_layers.2.dense.linear2.bias\", \"conv_layers.2.edge_update.norm.weight\", \"conv_layers.2.edge_update.norm.bias\", \"conv_layers.2.edge_update.EdgeMLP.0.weight\", \"conv_layers.2.edge_update.EdgeMLP.0.bias\", \"conv_layers.2.edge_update.EdgeMLP.2.weight\", \"conv_layers.2.edge_update.EdgeMLP.2.bias\", \"conv_layers.2.context.ContextMLP.0.weight\", \"conv_layers.2.context.ContextMLP.0.bias\", \"conv_layers.2.context.ContextMLP.2.weight\", \"conv_layers.2.context.ContextMLP.2.bias\", \"conv_layers.2.context.norm.weight\", \"conv_layers.2.context.norm.bias\", \"conv_layers.3.norm.0.weight\", \"conv_layers.3.norm.0.bias\", \"conv_layers.3.norm.1.weight\", \"conv_layers.3.norm.1.bias\", \"conv_layers.3.norm.2.weight\", \"conv_layers.3.norm.2.bias\", \"conv_layers.3.CrossAttn.query.weight\", \"conv_layers.3.CrossAttn.query.bias\", \"conv_layers.3.CrossAttn.key.weight\", \"conv_layers.3.CrossAttn.key.bias\", \"conv_layers.3.CrossAttn.value.weight\", \"conv_layers.3.CrossAttn.value.bias\", \"conv_layers.3.CrossAttn.out.weight\", \"conv_layers.3.CrossAttn.out.bias\", \"conv_layers.3.NeighAttn.W_Q.weight\", \"conv_layers.3.NeighAttn.W_K.weight\", \"conv_layers.3.NeighAttn.W_V.weight\", \"conv_layers.3.NeighAttn.W_O.weight\", \"conv_layers.3.dense.linear1.weight\", \"conv_layers.3.dense.linear1.bias\", \"conv_layers.3.dense.linear2.weight\", \"conv_layers.3.dense.linear2.bias\", \"conv_layers.3.edge_update.norm.weight\", \"conv_layers.3.edge_update.norm.bias\", \"conv_layers.3.edge_update.EdgeMLP.0.weight\", \"conv_layers.3.edge_update.EdgeMLP.0.bias\", \"conv_layers.3.edge_update.EdgeMLP.2.weight\", \"conv_layers.3.edge_update.EdgeMLP.2.bias\", \"conv_layers.3.context.ContextMLP.0.weight\", \"conv_layers.3.context.ContextMLP.0.bias\", \"conv_layers.3.context.ContextMLP.2.weight\", \"conv_layers.3.context.ContextMLP.2.bias\", \"conv_layers.3.context.norm.weight\", \"conv_layers.3.context.norm.bias\", \"out_mlp.0.weight\", \"out_mlp.0.bias\", \"out_mlp.2.weight\", \"out_mlp.2.bias\". "
     ]
    }
   ],
   "source": [
    "# Determine the best threshold for MCC based on the validation set.\n",
    "model_path = f'{root_path}/Output/{dataset}_5fold/' # if 5-fold cross-validation, {dataset}_5fold\n",
    "print(model_path)\n",
    "print(nn_config['pdb_class'])\n",
    "\n",
    "models = []\n",
    "for fold in range(5): # if 5-fold cross-validation, set to 5\n",
    "    state_dict = torch.load(model_path + 'fold%s.ckpt'%fold,'cuda:0')\n",
    "    model = LABind(\n",
    "        rfeat_dim=nn_config['rfeat_dim'], ligand_dim=nn_config['ligand_dim'], hidden_dim=nn_config['hidden_dim'], heads=nn_config['heads'], augment_eps=nn_config['augment_eps'], \n",
    "        rbf_num=nn_config['rbf_num'],top_k=nn_config['top_k'], attn_drop=nn_config['attn_drop'], dropout=nn_config['dropout'], num_layers=nn_config['num_layers']).to(DEVICE)\n",
    "    model = nn.DataParallel(model,device_ids=nn_config['device_ids'])\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "    \n",
    "valid_list = readDataList(f'{root_path}/{dataset}/label/picking.fa',skew=1)\n",
    "valid_data = readData.readData(\n",
    "    name_list=valid_list, \n",
    "    proj_dir=nn_config['proj_dir'], \n",
    "    lig_dict=nn_config['lig_dict'],\n",
    "    true_file=f'{root_path}/{dataset}/label/picking.fa', mode='train')\n",
    "# 打印长度\n",
    "valid_loader = DataLoader(valid_data, batch_size=nn_config['batch_size'], collate_fn=valid_data.collate_fn)\n",
    "print(f'valid data length: {len(valid_data)}')\n",
    "all_y_score = []\n",
    "all_y_true = []\n",
    "with torch.no_grad():\n",
    "    for rfeat, ligand, xyz,  mask, y_true in valid_loader:\n",
    "        tensors = [rfeat, ligand, xyz,  mask, y_true]\n",
    "        tensors = [tensor.to(DEVICE) for tensor in tensors]\n",
    "        rfeat, ligand, xyz, mask, y_true = tensors\n",
    "        \n",
    "        logits = [model(rfeat, ligand, xyz, mask).sigmoid() for model in models]\n",
    "        logits = torch.stack(logits,0).mean(0)\n",
    "        \n",
    "        logits = torch.masked_select(logits, mask==1)\n",
    "        y_true = torch.masked_select(y_true, mask==1)\n",
    "        all_y_score.extend(logits.cpu().detach().numpy())\n",
    "        all_y_true.extend(y_true.cpu().detach().numpy())\n",
    "\n",
    "best_threshold,best_mcc,best_pred = getBestThreshold(all_y_true, all_y_score)\n",
    "appendText(f'{model_path}/Best_Threshold.txt',f'{best_threshold} {best_mcc}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/virtual/zengzix4/LABind_ESM/Output/LigBind_5fold1/\n",
      "source\n",
      "CO3 test data length: 21\n",
      "SO4 test data length: 31\n",
      "Corrupt or mismatched file: 6ptkA\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m all_y_true \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m rfeat, ligand, xyz,  mask, y_true \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m rfeat \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/virtual/zengzix4/miniconda/envs/LABind/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/virtual/zengzix4/LABind_ESM/scripts/readData.py:45\u001b[0m, in \u001b[0;36mreadData.collate_fn\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcollate_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m     44\u001b[0m     features, ligands, xyzs, y_trues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m---> 45\u001b[0m     maxlen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1500\u001b[39m,\u001b[38;5;28mmax\u001b[39m([f\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]))\n\u001b[1;32m     46\u001b[0m     batch_feat \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     47\u001b[0m     batch_ligand \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/virtual/zengzix4/LABind_ESM/scripts/readData.py:45\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcollate_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m     44\u001b[0m     features, ligands, xyzs, y_trues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m---> 45\u001b[0m     maxlen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1500\u001b[39m,\u001b[38;5;28mmax\u001b[39m([\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]))\n\u001b[1;32m     46\u001b[0m     batch_feat \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     47\u001b[0m     batch_ligand \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "model_path = f'{root_path}/Output/{dataset}_5fold1/' # if 5-fold cross-validation, {dataset}_5fold\n",
    "print(model_path)\n",
    "print(nn_config['pdb_class'])\n",
    "\n",
    "models = []\n",
    "for fold in range(5): # if 5-fold cross-validation, set to 5\n",
    "    state_dict = torch.load(model_path + 'fold%s.ckpt'%fold,'cuda:0')\n",
    "    model = LABind(\n",
    "        rfeat_dim=nn_config['rfeat_dim'], ligand_dim=nn_config['ligand_dim'], hidden_dim=nn_config['hidden_dim'], heads=nn_config['heads'], augment_eps=nn_config['augment_eps'], \n",
    "        rbf_num=nn_config['rbf_num'],top_k=nn_config['top_k'], attn_drop=nn_config['attn_drop'], dropout=nn_config['dropout'], num_layers=nn_config['num_layers']).to(DEVICE)\n",
    "    model = nn.DataParallel(model,device_ids=nn_config['device_ids'])\n",
    "    \n",
    "    for k, v in state_dict.items():\n",
    "        # if checkpoint lacks \"module.\", add it\n",
    "        if not k.startswith(\"module.\"):\n",
    "            new_state_dict[\"module.\" + k] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "\n",
    "df = pd.DataFrame(columns=['ligand','Rec','SPE','Acc','Pre','F1','MCC','AUC','AUPR'])\n",
    "for ionic in os.listdir(f'{root_path}/{dataset}/label/test/'):\n",
    "    ionic = ionic.split('.')[0]\n",
    "    test_list = readDataList(f'{root_path}/{dataset}/label/test/{ionic}.fa',skew=1)\n",
    "    test_data = readData.readData(\n",
    "        name_list=test_list, \n",
    "        proj_dir=nn_config['proj_dir'], \n",
    "        lig_dict=nn_config['lig_dict'],\n",
    "        true_file=f'{root_path}/{dataset}/label/test/{ionic}.fa', mode='test')\n",
    "    # 打印长度\n",
    "    test_loader = DataLoader(test_data, batch_size=nn_config['batch_size'], collate_fn=test_data.collate_fn)\n",
    "    print(f'{ionic} test data length: {len(test_data)}')\n",
    "    all_y_score = []\n",
    "    all_y_true = []\n",
    "    with torch.no_grad():\n",
    "        for rfeat, ligand, xyz,  mask, y_true in test_loader:\n",
    "            if rfeat == None:\n",
    "                continue\n",
    "            tensors = [rfeat, ligand, xyz,  mask, y_true]\n",
    "            tensors = [tensor.to(DEVICE) for tensor in tensors]\n",
    "            rfeat, ligand, xyz, mask, y_true = tensors\n",
    "            \n",
    "            logits = [model(rfeat, ligand, xyz, mask).sigmoid() for model in models]\n",
    "            logits = torch.stack(logits,0).mean(0)\n",
    "            \n",
    "            logits = torch.masked_select(logits, mask==1)\n",
    "            y_true = torch.masked_select(y_true, mask==1)\n",
    "            all_y_score.extend(logits.cpu().detach().numpy())\n",
    "            all_y_true.extend(y_true.cpu().detach().numpy())\n",
    "    data_dict = calEval(all_y_true, all_y_score) # please set best threshold.\n",
    "    data_dict['ligand'] = ionic\n",
    "    df = pd.concat([df,pd.DataFrame(data_dict,index=[0])])\n",
    "df.to_csv(f'{model_path}test.csv',index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training done, no validation though\n",
    "also "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LABind)",
   "language": "python",
   "name": "labind"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
